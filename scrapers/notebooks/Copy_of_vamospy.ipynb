{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Places Api"
      ],
      "metadata": {
        "id": "H7kRUeA4gn16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZTi-go-qoSS"
      },
      "outputs": [],
      "source": [
        "# Part 1: Setup and Initialization\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "\n",
        "# Constants\n",
        "API_KEY = 'AIzaSyCjrwh-MIXJDbM-9K7NmMF2TyfTZMuNgPA'  # Replace with your actual API key\n",
        "LOCATION = '-25.2637,-57.5759'  # Asunción coordinates\n",
        "RADIUS = 15000  # 5 km radius # Categories to fetch\n",
        "OUTPUT_FILE = 'places_details.csv'\n",
        "\n",
        "# Headers for the CSV file\n",
        "CSV_HEADERS = ['place_id', 'place_name', 'category', 'rating', 'location_lat', 'location_lng', 'formatted_address', 'opening_hours', 'phone_number', 'website_url', 'facebook', 'instagram', 'twitter', 'timestamp']\n",
        "\n",
        "# # Function to write data to CSV\n",
        "# def write_to_csv(data, file):\n",
        "#     with open(file, mode='a', newline='', encoding='utf-8') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow(data)\n",
        "\n",
        "# # Initialize CSV with headers\n",
        "# with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow(CSV_HEADERS)\n",
        "\n",
        "# print(\"Setup complete and CSV initialized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDfE8SaHtMGb"
      },
      "outputs": [],
      "source": [
        "PLACE_TYPES = [\n",
        "    'accounting', 'airport', 'amusement_park', 'aquarium', 'art_gallery', 'atm', 'bakery', 'bank', 'bar',\n",
        "    'beauty_salon', 'bicycle_store', 'book_store', 'bowling_alley', 'bus_station', 'cafe', 'campground',\n",
        "    'car_dealer', 'car_rental', 'car_repair', 'car_wash', 'casino', 'cemetery', 'church', 'city_hall',\n",
        "    'clothing_store', 'convenience_store', 'courthouse', 'dentist', 'department_store', 'doctor', 'electrician',\n",
        "    'electronics_store', 'embassy', 'fire_station', 'florist', 'funeral_home', 'furniture_store', 'gas_station',\n",
        "    'gym', 'hair_care', 'hardware_store', 'hindu_temple', 'home_goods_store', 'hospital', 'insurance_agency',\n",
        "    'jewelry_store', 'laundry', 'lawyer', 'library', 'light_rail_station', 'liquor_store', 'local_government_office',\n",
        "    'locksmith', 'lodging', 'meal_delivery', 'meal_takeaway', 'mosque', 'movie_rental', 'movie_theater',\n",
        "    'moving_company', 'museum', 'night_club', 'painter', 'park', 'parking', 'pet_store', 'pharmacy', 'physiotherapist',\n",
        "    'plumber', 'police', 'post_office', 'real_estate_agency', 'restaurant', 'roofing_contractor', 'rv_park', 'school',\n",
        "    'shoe_store', 'shopping_mall', 'spa', 'stadium', 'storage', 'store', 'subway_station', 'supermarket', 'synagogue',\n",
        "    'taxi_stand', 'train_station', 'transit_station', 'travel_agency', 'university', 'veterinary_care', 'zoo'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d15T8kzzgjam"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Sm2enbyoqs1C"
      },
      "outputs": [],
      "source": [
        "def get_place_details(place_id):\n",
        "    place_details_url = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&fields=name,rating,formatted_address,geometry,opening_hours,international_phone_number,website&key={API_KEY}\"\n",
        "\n",
        "    # Debugging: Print the place details request URL\n",
        "    print(f\"Fetching details for place_id: {place_id}\")\n",
        "    print(f\"Details URL: {place_details_url}\")\n",
        "\n",
        "    response = requests.get(place_details_url)\n",
        "\n",
        "    # Debugging: Check the status code and response\n",
        "    print(f\"Response Status Code for Place Details: {response.status_code}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get('result', {})\n",
        "    else:\n",
        "        print(f\"Error fetching details for place_id {place_id}. Status code: {response.status_code}\")\n",
        "        return {}\n",
        "\n",
        "# Function to get nearby places with debugging\n",
        "def get_nearby_places():\n",
        "    places = []\n",
        "    for place_type in PLACE_TYPES:\n",
        "        nearby_search_url = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={LOCATION}&radius={RADIUS}&type={place_type}&key={API_KEY}\"\n",
        "\n",
        "        # Debugging: Print the URL to ensure it's correct\n",
        "        print(f\"Fetching places of type: {place_type}\")\n",
        "        print(f\"Request URL: {nearby_search_url}\")\n",
        "\n",
        "        response = requests.get(nearby_search_url)\n",
        "\n",
        "        # Debugging: Check the status code of the response\n",
        "        print(f\"Response Status Code: {response.status_code}\")\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            # Debugging: Print the full response to understand the content\n",
        "            print(\"Response JSON:\")\n",
        "            print(data)\n",
        "\n",
        "            if data.get('results'):\n",
        "                places.extend(data['results'])\n",
        "            else:\n",
        "                print(f\"No places found for type: {place_type}\")\n",
        "        else:\n",
        "            # If the request failed, print the error\n",
        "            print(f\"Error fetching places for type {place_type}. Status code: {response.status_code}\")\n",
        "\n",
        "    return places\n",
        "\n",
        "# Function to write data to CSV\n",
        "def write_to_csv(data, file):\n",
        "    with open(file, mode='a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(data)\n",
        "\n",
        "# Initialize CSV with headers\n",
        "with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(CSV_HEADERS)\n",
        "\n",
        "# Main process: Fetch nearby places, then get details and write to CSV\n",
        "def main():\n",
        "    places = get_nearby_places()\n",
        "\n",
        "    print(f\"Total places fetched: {len(places)}\")\n",
        "\n",
        "    if len(places) > 0:\n",
        "        for place in places:\n",
        "            place_id = place.get('place_id')\n",
        "            details = get_place_details(place_id)\n",
        "\n",
        "            # Extract relevant fields from place details\n",
        "            place_name = details.get('name', 'N/A')\n",
        "            rating = details.get('rating', 'N/A')\n",
        "            formatted_address = details.get('formatted_address', 'N/A')\n",
        "            location_lat = details['geometry']['location']['lat']\n",
        "            location_lng = details['geometry']['location']['lng']\n",
        "            opening_hours = details.get('opening_hours', {}).get('weekday_text', 'N/A')\n",
        "            phone_number = details.get('international_phone_number', details.get('national_phone_number', 'N/A'))\n",
        "            website_url = details.get('website', 'N/A')\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
        "\n",
        "            # Determine the category (place type)\n",
        "            categories = place.get('types', [])\n",
        "            category = categories[0] if categories else 'N/A'\n",
        "\n",
        "            # Write the row to the CSV (without social media links)\n",
        "            row = [place_id, place_name, category, rating, location_lat, location_lng, formatted_address, opening_hours, phone_number, website_url, timestamp]\n",
        "            write_to_csv(row, OUTPUT_FILE)\n",
        "\n",
        "    else:\n",
        "        print(\"No places were fetched.\")\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lsJ-UKrqujv"
      },
      "outputs": [],
      "source": [
        "# Part 3: Fetch Place Details and Extract Social Media Links\n",
        "\n",
        "# Function to get detailed information about each place\n",
        "def get_place_details(place_id):\n",
        "    place_details_url = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place_id}&fields=name,rating,formatted_address,geometry,opening_hours,international_phone_number,website&key={API_KEY}\"\n",
        "    response = requests.get(place_details_url)\n",
        "    return response.json().get('result', {})\n",
        "\n",
        "# Function to extract social media links from a website\n",
        "def extract_social_media_links(website_url):\n",
        "    social_media = {'facebook': None, 'instagram': None, 'twitter': None}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(website_url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Look for links in the website's content\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "                if 'facebook.com' in href:\n",
        "                    social_media['facebook'] = href\n",
        "                elif 'instagram.com' in href:\n",
        "                    social_media['instagram'] = href\n",
        "                elif 'twitter.com' in href:\n",
        "                    social_media['twitter'] = href\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching website {website_url}: {e}\")\n",
        "\n",
        "    return social_media\n",
        "\n",
        "# Process each place\n",
        "for place in places:\n",
        "    place_id = place.get('place_id')\n",
        "    details = get_place_details(place_id)\n",
        "\n",
        "    # Extract relevant fields\n",
        "    place_name = details.get('name', 'N/A')\n",
        "    rating = details.get('rating', 'N/A')\n",
        "    formatted_address = details.get('formatted_address', 'N/A')\n",
        "    location_lat = details['geometry']['location']['lat']\n",
        "    location_lng = details['geometry']['location']['lng']\n",
        "    opening_hours = details.get('opening_hours', {}).get('weekday_text', 'N/A')\n",
        "    phone_number = details.get('international_phone_number', details.get('national_phone_number', 'N/A'))\n",
        "    website_url = details.get('website', 'N/A')\n",
        "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime())\n",
        "\n",
        "    # Determine the category (place type)\n",
        "    categories = place.get('types', [])\n",
        "    category = categories[0] if categories else 'N/A'\n",
        "\n",
        "    # Extract social media links (if a website is available)\n",
        "    facebook, instagram, twitter = None, None, None\n",
        "    if website_url != 'N/A':\n",
        "        social_media_links = extract_social_media_links(website_url)\n",
        "        facebook = social_media_links.get('facebook')\n",
        "        instagram = social_media_links.get('instagram')\n",
        "        twitter = social_media_links.get('twitter')\n",
        "\n",
        "    # Write the row to the CSV\n",
        "    row = [place_id, place_name, category, rating, location_lat, location_lng, formatted_address, opening_hours, phone_number, website_url, facebook, instagram, twitter, timestamp]\n",
        "    write_to_csv(row, OUTPUT_FILE)\n",
        "\n",
        "print(f\"Data written to {OUTPUT_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ticketea Scraping\n"
      ],
      "metadata": {
        "id": "_R7TQwYtSDcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def fetch_event_links():\n",
        "    base_url = \"https://ticketea.com.py\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        print(f\"Status code for main page: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching main page: {e}\")\n",
        "        return []\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch the main page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    print(f\"Page content length: {len(response.content)}\")\n",
        "\n",
        "    event_links = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if href.startswith(\"/events/\"):\n",
        "            full_url = f\"{base_url}{href}\"\n",
        "            event_links.append(full_url)\n",
        "            print(f\"Found event link: {full_url}\")\n",
        "\n",
        "    event_links = list(set(event_links))\n",
        "    print(f\"Total unique event links: {len(event_links)}\")\n",
        "    return event_links\n",
        "\n",
        "def extract_meta_property(soup, property_name):\n",
        "    meta_tag = soup.find('meta', property=property_name)\n",
        "    return meta_tag.get('content') if meta_tag else None\n",
        "\n",
        "def extract_coordinates_from_iframe(soup):\n",
        "    iframe = soup.find('iframe', src=lambda x: x and 'google.com/maps/embed' in x)\n",
        "    if iframe and iframe.get('src'):\n",
        "        src = iframe['src']\n",
        "        if 'q=' in src:\n",
        "            coords = src.split('q=')[-1].split('&')[0]\n",
        "            try:\n",
        "                lat, lon = map(float, coords.split(','))\n",
        "                return {'latitude': lat, 'longitude': lon}\n",
        "            except:\n",
        "                pass\n",
        "    return {'latitude': None, 'longitude': None}\n",
        "\n",
        "def fetch_event_details(event_url):\n",
        "    print(f\"\\nFetching details for event: {event_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(event_url)\n",
        "        print(f\"Status code for event page {event_url}: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching event page {event_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch event details for {event_url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    print(f\"Event page content length: {len(response.content)}\")\n",
        "\n",
        "    # Extract meta properties\n",
        "    meta_data = {\n",
        "        'id': extract_meta_property(soup, 'og:id'),\n",
        "        'category': extract_meta_property(soup, 'og:product:category'),\n",
        "        'price': extract_meta_property(soup, 'og:product:price:amount'),\n",
        "        'currency': extract_meta_property(soup, 'og:product:price:currency'),\n",
        "        'brand': extract_meta_property(soup, 'og:product:brand'),\n",
        "        'availability': extract_meta_property(soup, 'og:product:availability'),\n",
        "        'condition': extract_meta_property(soup, 'og:product:condition')\n",
        "    }\n",
        "\n",
        "    # Extract title from different possible locations\n",
        "    title = soup.find('title').text.strip() if soup.find('title') else None\n",
        "    if title and \"Entradas para\" in title:\n",
        "        title = title.replace(\"Entradas para\", \"\").replace(\"- Ticketea.com.py\", \"\").strip()\n",
        "\n",
        "    # Extract coordinates\n",
        "    coordinates = extract_coordinates_from_iframe(soup)\n",
        "\n",
        "    # Extract event card details\n",
        "    event_card = soup.find(\"div\", {\"class\": \"card card-event-config\"})\n",
        "    if event_card:\n",
        "        print(\"Found the event card.\")\n",
        "\n",
        "        # Extract event image attributes\n",
        "        image_tag = event_card.find(\"img\")\n",
        "        img_alt = image_tag.get('alt', 'No Alt') if image_tag else \"No Alt\"\n",
        "        img_title = image_tag.get('title', 'No Title') if image_tag else \"No Title\"\n",
        "        img_src = image_tag.get('src', 'No Image') if image_tag else \"No Image\"\n",
        "\n",
        "        # Extract event info\n",
        "        event_info = event_card.find_all(\"p\", {\"class\": \"size-md m-b-0 line-height-20\"})\n",
        "        event_hours = event_info[0].text.strip() if len(event_info) > 0 else \"No Hours\"\n",
        "        event_location = event_info[1].text.strip() if len(event_info) > 1 else \"No Location\"\n",
        "        event_address = event_info[2].text.strip() if len(event_info) > 2 else \"No Address\"\n",
        "\n",
        "        # Extract description\n",
        "        details_elem = soup.find(\"div\", {\"class\": \"event__detalle p-x\"})\n",
        "        additional_details = []\n",
        "        if details_elem:\n",
        "            for p in details_elem.find_all(\"p\"):\n",
        "                additional_details.append(p.text.strip())\n",
        "        additional_details_text = \" | \".join(additional_details)\n",
        "\n",
        "        # Compile all extracted data\n",
        "        event_data = {\n",
        "            \"title\": title,\n",
        "            \"image_alt\": img_alt,\n",
        "            \"image_title\": img_title,\n",
        "            \"image_url\": img_src,\n",
        "            \"event_url\": event_url,\n",
        "            \"event_hours\": event_hours,\n",
        "            \"event_location\": event_location,\n",
        "            \"event_address\": event_address,\n",
        "            \"additional_details\": additional_details_text,\n",
        "            \"coordinates\": coordinates,\n",
        "            \"meta\": meta_data\n",
        "        }\n",
        "\n",
        "        return event_data\n",
        "    else:\n",
        "        print(f\"No event card found for {event_url}.\")\n",
        "        return None\n",
        "\n",
        "def write_to_csv(events, timestamp):\n",
        "    if not events:\n",
        "        print(\"No event data available to write.\")\n",
        "        return\n",
        "\n",
        "    output_dir = \"output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    csv_path = os.path.join(output_dir, f\"ticketea_events_{timestamp}.csv\")\n",
        "    json_path = os.path.join(output_dir, f\"ticketea_events_{timestamp}.json\")\n",
        "\n",
        "    # Write CSV\n",
        "    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=events[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(events)\n",
        "    print(f\"CSV file saved: {csv_path}\")\n",
        "\n",
        "    # Write JSON\n",
        "    with open(json_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(events, file, ensure_ascii=False, indent=2)\n",
        "    print(f\"JSON file saved: {json_path}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\n=== Starting event scraping process... ===\\n\")\n",
        "\n",
        "    event_links = fetch_event_links()\n",
        "    print(f\"\\nFound {len(event_links)} event links.\")\n",
        "\n",
        "    events = []\n",
        "    for event_url in event_links:\n",
        "        print(f\"\\nScraping event: {event_url}\")\n",
        "        event_data = fetch_event_details(event_url)\n",
        "\n",
        "        if event_data:\n",
        "            events.append(event_data)\n",
        "            print(f\"Successfully scraped: {event_data['title']}\")\n",
        "\n",
        "        time.sleep(2)  # Sleep between requests\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    if events:\n",
        "        write_to_csv(events, timestamp)\n",
        "    else:\n",
        "        print(\"No events to write to files.\")\n",
        "\n",
        "    print(f\"\\n=== Scraping completed. Total events scraped: {len(events)} ===\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6HyBs9Z5d5k-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9c8580-1d88-4693-87dd-3ca75d418643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting event scraping process... ===\n",
            "\n",
            "Status code for main page: 200\n",
            "Page content length: 138857\n",
            "Found event link: https://ticketea.com.py/events/duki-ameri-world-tour-2025-dia-2\n",
            "Found event link: https://ticketea.com.py/events/rock-al-puerto\n",
            "Found event link: https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025\n",
            "Found event link: https://ticketea.com.py/events/river-beats\n",
            "Found event link: https://ticketea.com.py/events/conferencia-mario-alonso-puig\n",
            "Found event link: https://ticketea.com.py/events/paraguari-music-fest-2\n",
            "Found event link: https://ticketea.com.py/events/kany-garcia-en-paraguay\n",
            "Found event link: https://ticketea.com.py/events/congreso-diamante-renuevo-el-pacto\n",
            "Found event link: https://ticketea.com.py/events/un-mentes-expertas-de-victor-kuppers-vivir-con-actitud\n",
            "Found event link: https://ticketea.com.py/events/ventas-salvajes\n",
            "Found event link: https://ticketea.com.py/events/retro-fest\n",
            "Found event link: https://ticketea.com.py/events/el-retrovisor-dia-del-trabajador\n",
            "Found event link: https://ticketea.com.py/events/wave-x-kobosil\n",
            "Found event link: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-asuncion\n",
            "Found event link: https://ticketea.com.py/events/moonlight-showcase-tango\n",
            "Found event link: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-ciudad-del-este\n",
            "Found event link: https://ticketea.com.py/events/asi-de-simple-2025\n",
            "Found event link: https://ticketea.com.py/events/permitidos\n",
            "Found event link: https://ticketea.com.py/events/duki-ameri-world-tour-2025\n",
            "Found event link: https://ticketea.com.py/events/duki-ameri-world-tour-2025-dia-2\n",
            "Found event link: https://ticketea.com.py/events/river-beats\n",
            "Found event link: https://ticketea.com.py/events/wave-x-kobosil\n",
            "Found event link: https://ticketea.com.py/events/congreso-diamante-renuevo-el-pacto\n",
            "Found event link: https://ticketea.com.py/events/el-retrovisor-dia-del-trabajador\n",
            "Found event link: https://ticketea.com.py/events/rock-al-puerto\n",
            "Found event link: https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025\n",
            "Found event link: https://ticketea.com.py/events/asi-de-simple-2025\n",
            "Found event link: https://ticketea.com.py/events/permitidos\n",
            "Found event link: https://ticketea.com.py/events/duki-ameri-world-tour-2025\n",
            "Found event link: https://ticketea.com.py/events/duki-ameri-world-tour-2025-dia-2\n",
            "Found event link: https://ticketea.com.py/events/mamma-mia\n",
            "Found event link: https://ticketea.com.py/events/arlandria-presenta-cabezones-edu-schmidt-ex-arbol-vartan-los-ladrillos\n",
            "Found event link: https://ticketea.com.py/events/moonlight-showcase-tango\n",
            "Found event link: https://ticketea.com.py/events/dos-22-presenta-cosmic-boys\n",
            "Found event link: https://ticketea.com.py/events/una-historia-de-oz\n",
            "Found event link: https://ticketea.com.py/events/river-beats\n",
            "Found event link: https://ticketea.com.py/events/concepcion-music-fest\n",
            "Found event link: https://ticketea.com.py/events/wave-x-kobosil\n",
            "Found event link: https://ticketea.com.py/events/paraguari-music-fest-2\n",
            "Found event link: https://ticketea.com.py/events/retro-fest\n",
            "Found event link: https://ticketea.com.py/events/kany-garcia-en-paraguay\n",
            "Found event link: https://ticketea.com.py/events/congreso-diamante-renuevo-el-pacto\n",
            "Found event link: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-asuncion\n",
            "Found event link: https://ticketea.com.py/events/el-retrovisor-dia-del-trabajador\n",
            "Found event link: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-ciudad-del-este\n",
            "Found event link: https://ticketea.com.py/events/rock-al-puerto\n",
            "Found event link: https://ticketea.com.py/events/conferencia-mario-alonso-puig\n",
            "Found event link: https://ticketea.com.py/events/ventas-salvajes\n",
            "Found event link: https://ticketea.com.py/events/un-mentes-expertas-de-victor-kuppers-vivir-con-actitud\n",
            "Found event link: https://ticketea.com.py/events/expo-vino-2025\n",
            "Found event link: https://ticketea.com.py/events/the-messengers-conference-con-lucas-magnin-y-kike-pavon\n",
            "Found event link: https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025\n",
            "Found event link: https://ticketea.com.py/events/congreso-internacional-eduvet\n",
            "Total unique event links: 27\n",
            "\n",
            "Found 27 event links.\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/river-beats\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/river-beats\n",
            "Status code for event page https://ticketea.com.py/events/river-beats: 200\n",
            "Event page content length: 56219\n",
            "Found the event card.\n",
            "Successfully scraped: RIVER BEATS\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/dos-22-presenta-cosmic-boys\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/dos-22-presenta-cosmic-boys\n",
            "Status code for event page https://ticketea.com.py/events/dos-22-presenta-cosmic-boys: 200\n",
            "Event page content length: 57612\n",
            "Found the event card.\n",
            "Successfully scraped: DOS-22 Presenta COSMIC BOYS\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/retro-fest\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/retro-fest\n",
            "Status code for event page https://ticketea.com.py/events/retro-fest: 200\n",
            "Event page content length: 54047\n",
            "Found the event card.\n",
            "Successfully scraped: RETRO FEST\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/kany-garcia-en-paraguay\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/kany-garcia-en-paraguay\n",
            "Status code for event page https://ticketea.com.py/events/kany-garcia-en-paraguay: 200\n",
            "Event page content length: 56077\n",
            "Found the event card.\n",
            "Successfully scraped: KANY GARCIA EN PARAGUAY\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/ventas-salvajes\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/ventas-salvajes\n",
            "Status code for event page https://ticketea.com.py/events/ventas-salvajes: 200\n",
            "Event page content length: 58588\n",
            "Found the event card.\n",
            "Successfully scraped: VENTAS SALVAJES\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/el-retrovisor-dia-del-trabajador\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/el-retrovisor-dia-del-trabajador\n",
            "Status code for event page https://ticketea.com.py/events/el-retrovisor-dia-del-trabajador: 200\n",
            "Event page content length: 58438\n",
            "Found the event card.\n",
            "Successfully scraped: EL RETROVISOR - DIA DEL TRABAJADOR\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/una-historia-de-oz\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/una-historia-de-oz\n",
            "Status code for event page https://ticketea.com.py/events/una-historia-de-oz: 200\n",
            "Event page content length: 56552\n",
            "Found the event card.\n",
            "Successfully scraped: UNA HISTORIA DE OZ\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/moonlight-showcase-tango\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/moonlight-showcase-tango\n",
            "Status code for event page https://ticketea.com.py/events/moonlight-showcase-tango: 200\n",
            "Event page content length: 57143\n",
            "Found the event card.\n",
            "Successfully scraped: MOONLIGHT SHOWCASE TANGO\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/congreso-diamante-renuevo-el-pacto\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/congreso-diamante-renuevo-el-pacto\n",
            "Status code for event page https://ticketea.com.py/events/congreso-diamante-renuevo-el-pacto: 200\n",
            "Event page content length: 60239\n",
            "Found the event card.\n",
            "Successfully scraped: CONGRESO DIAMANTE 2025 –“ Renuevo el Pacto”\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/mamma-mia\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/mamma-mia\n",
            "Status code for event page https://ticketea.com.py/events/mamma-mia: 200\n",
            "Event page content length: 57164\n",
            "Found the event card.\n",
            "Successfully scraped: MAMMA MIA\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/the-messengers-conference-con-lucas-magnin-y-kike-pavon\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/the-messengers-conference-con-lucas-magnin-y-kike-pavon\n",
            "Status code for event page https://ticketea.com.py/events/the-messengers-conference-con-lucas-magnin-y-kike-pavon: 200\n",
            "Event page content length: 59162\n",
            "Found the event card.\n",
            "Successfully scraped: THE MESSENGERS CONFERENCE CON LUCAS MAGNIN Y KIKE PAVON\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/wave-x-kobosil\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/wave-x-kobosil\n",
            "Status code for event page https://ticketea.com.py/events/wave-x-kobosil: 200\n",
            "Event page content length: 57157\n",
            "Found the event card.\n",
            "Successfully scraped: WAVE x KOBOSIL\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/paraguari-music-fest-2\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/paraguari-music-fest-2\n",
            "Status code for event page https://ticketea.com.py/events/paraguari-music-fest-2: 200\n",
            "Event page content length: 54351\n",
            "Found the event card.\n",
            "Successfully scraped: PARAGUARI MUSIC FEST 2\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-ciudad-del-este\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-ciudad-del-este\n",
            "Status code for event page https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-ciudad-del-este: 200\n",
            "Event page content length: 57666\n",
            "Found the event card.\n",
            "Successfully scraped: DOS-22 Presenta CRISTOBAL PESCE (Ciudad del este)\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/permitidos\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/permitidos\n",
            "Status code for event page https://ticketea.com.py/events/permitidos: 200\n",
            "Event page content length: 54000\n",
            "Found the event card.\n",
            "Successfully scraped: PERMITIDOS\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/asi-de-simple-2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/asi-de-simple-2025\n",
            "Status code for event page https://ticketea.com.py/events/asi-de-simple-2025: 200\n",
            "Event page content length: 59360\n",
            "Found the event card.\n",
            "Successfully scraped: ASI DE SIMPLE 2025\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/un-mentes-expertas-de-victor-kuppers-vivir-con-actitud\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/un-mentes-expertas-de-victor-kuppers-vivir-con-actitud\n",
            "Status code for event page https://ticketea.com.py/events/un-mentes-expertas-de-victor-kuppers-vivir-con-actitud: 200\n",
            "Event page content length: 65214\n",
            "Found the event card.\n",
            "Successfully scraped: UN MENTES EXPERTAS DE VICTOR KÜPPERS – VIVIR CON ACTITUD\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/rock-al-puerto\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/rock-al-puerto\n",
            "Status code for event page https://ticketea.com.py/events/rock-al-puerto: 200\n",
            "Event page content length: 67905\n",
            "Found the event card.\n",
            "Successfully scraped: ROCK AL PUERTO\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/expo-vino-2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/expo-vino-2025\n",
            "Status code for event page https://ticketea.com.py/events/expo-vino-2025: 200\n",
            "Event page content length: 60991\n",
            "Found the event card.\n",
            "Successfully scraped: EXPO VINO 2025\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-asuncion\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-asuncion\n",
            "Status code for event page https://ticketea.com.py/events/dos-22-presenta-cristobal-pesce-asuncion: 200\n",
            "Event page content length: 57240\n",
            "Found the event card.\n",
            "Successfully scraped: DOS-22 Presenta CRISTOBAL PESCE (Asuncion)\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025\n",
            "Status code for event page https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025: 200\n",
            "Event page content length: 63546\n",
            "Found the event card.\n",
            "Successfully scraped: CHAYANNE - BAILEMOS OTRA VEZ TOUR 2025\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/duki-ameri-world-tour-2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/duki-ameri-world-tour-2025\n",
            "Status code for event page https://ticketea.com.py/events/duki-ameri-world-tour-2025: 200\n",
            "Event page content length: 69779\n",
            "Found the event card.\n",
            "Successfully scraped: DUKI - AMERI WORLD TOUR 2025 - DIA 1\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/conferencia-mario-alonso-puig\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/conferencia-mario-alonso-puig\n",
            "Status code for event page https://ticketea.com.py/events/conferencia-mario-alonso-puig: 200\n",
            "Event page content length: 61048\n",
            "Found the event card.\n",
            "Successfully scraped: CONFERENCIA MARIO ALONSO PUIG\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/arlandria-presenta-cabezones-edu-schmidt-ex-arbol-vartan-los-ladrillos\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/arlandria-presenta-cabezones-edu-schmidt-ex-arbol-vartan-los-ladrillos\n",
            "Status code for event page https://ticketea.com.py/events/arlandria-presenta-cabezones-edu-schmidt-ex-arbol-vartan-los-ladrillos: 200\n",
            "Event page content length: 63042\n",
            "Found the event card.\n",
            "Successfully scraped: ARLANDRIA PRESENTA\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/concepcion-music-fest\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/concepcion-music-fest\n",
            "Status code for event page https://ticketea.com.py/events/concepcion-music-fest: 200\n",
            "Event page content length: 56261\n",
            "Found the event card.\n",
            "Successfully scraped: CONCEPCION MUSIC FEST\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/duki-ameri-world-tour-2025-dia-2\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/duki-ameri-world-tour-2025-dia-2\n",
            "Status code for event page https://ticketea.com.py/events/duki-ameri-world-tour-2025-dia-2: 200\n",
            "Event page content length: 69680\n",
            "Found the event card.\n",
            "Successfully scraped: DUKI - AMERI WORLD TOUR 2025 - DIA 2\n",
            "\n",
            "Scraping event: https://ticketea.com.py/events/congreso-internacional-eduvet\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/congreso-internacional-eduvet\n",
            "Status code for event page https://ticketea.com.py/events/congreso-internacional-eduvet: 200\n",
            "Event page content length: 58322\n",
            "Found the event card.\n",
            "Successfully scraped: CONGRESO INTERNACIONAL EDUVET\n",
            "CSV file saved: output/ticketea_events_20250403_120346.csv\n",
            "JSON file saved: output/ticketea_events_20250403_120346.json\n",
            "\n",
            "=== Scraping completed. Total events scraped: 27 ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from typing import Dict, Optional, Tuple\n",
        "from googlesearch import search\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def extract_coordinates(html_content: str) -> Optional[Tuple[float, float]]:\n",
        "    \"\"\"Extract coordinates from the map link in the HTML content.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    map_link = soup.find('a', {'aria-label': 'View larger map'})\n",
        "\n",
        "    if map_link and 'll=' in map_link.get('href', ''):\n",
        "        href = map_link['href']\n",
        "        coords_match = re.search(r'll=([-\\d.]+),([-\\d.]+)', href)\n",
        "        if coords_match:\n",
        "            lat, lng = map(float, coords_match.groups())\n",
        "            return (lat, lng)\n",
        "    return None\n",
        "\n",
        "def determine_event_category(event_data: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Determine event category based on predefined local categories.\n",
        "    \"\"\"\n",
        "    # Define category keywords in both Spanish and English\n",
        "    category_keywords = {\n",
        "        'Capacitaciones': [\n",
        "            'capacitación', 'taller', 'curso', 'workshop', 'training',\n",
        "            'aprende', 'aprendizaje', 'certificación', 'diploma'\n",
        "        ],\n",
        "        'Poesía': [\n",
        "            'poesía', 'poetry', 'poeta', 'verso', 'recital',\n",
        "            'poema', 'lírica', 'literario'\n",
        "        ],\n",
        "        'Conferencias': [\n",
        "            'conferencia', 'charla', 'ponencia', 'seminario', 'congreso',\n",
        "            'lecture', 'conference', 'talk', 'symposium'\n",
        "        ],\n",
        "        'Festivales': [\n",
        "            'festival', 'feria', 'fiesta', 'celebration', 'carnaval',\n",
        "            'fair', 'fest'\n",
        "        ],\n",
        "        'Cine': [\n",
        "            'cine', 'película', 'film', 'cinema', 'movie',\n",
        "            'proyección', 'screening', 'premiere', 'estreno'\n",
        "        ],\n",
        "        'Música': [\n",
        "            'música', 'music', 'concierto', 'concert', 'show',\n",
        "            'banda', 'band', 'cantante', 'singer', 'dj', 'recital',\n",
        "            'orquesta', 'orchestra', 'musical'\n",
        "        ],\n",
        "        'Danza': [\n",
        "            'danza', 'dance', 'baile', 'ballet', 'coreografía',\n",
        "            'choreography', 'bailarín', 'dancer'\n",
        "        ],\n",
        "        'Teatro': [\n",
        "            'teatro', 'theater', 'obra', 'play', 'drama',\n",
        "            'actuación', 'acting', 'theatrical', 'teatral'\n",
        "        ],\n",
        "        'Arte': [\n",
        "            'arte', 'art', 'exposición', 'exhibition', 'galería',\n",
        "            'gallery', 'artista', 'artist', 'muestra'\n",
        "        ],\n",
        "        'Fotografía': [\n",
        "            'fotografía', 'photography', 'foto', 'photo', 'imagen',\n",
        "            'fotógrafo', 'photographer', 'photoshoot'\n",
        "        ],\n",
        "        'Gastronomía': [\n",
        "            'gastronomía', 'gastronomy', 'comida', 'food', 'cocina',\n",
        "            'cooking', 'culinario', 'culinary', 'restaurante', 'degustación',\n",
        "            'tasting', 'chef'\n",
        "        ],\n",
        "        'Deportes': [\n",
        "            'deporte', 'sports', 'partido', 'match', 'torneo',\n",
        "            'tournament', 'campeonato', 'championship', 'competencia',\n",
        "            'competition', 'juego', 'game'\n",
        "        ],\n",
        "        'Turismo': [\n",
        "            'turismo', 'tourism', 'viaje', 'travel', 'excursión',\n",
        "            'tour', 'visita', 'visit', 'aventura', 'adventure'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Combine event name and description for analysis\n",
        "        event_text = f\"{event_data['eventName']} {event_data.get('eventDescription', '')}\".lower()\n",
        "\n",
        "        # Search for keywords in the event text\n",
        "        category_scores = {}\n",
        "        for category, keywords in category_keywords.items():\n",
        "            # Count matches for each keyword\n",
        "            score = sum(1 for keyword in keywords if keyword.lower() in event_text)\n",
        "            # Add extra weight for keywords found in the event name\n",
        "            name_score = sum(3 for keyword in keywords\n",
        "                           if keyword.lower() in event_data['eventName'].lower())\n",
        "            category_scores[category] = score + name_score\n",
        "\n",
        "        # Get additional context from web search\n",
        "        search_query = f\"{event_data['eventName']} {event_data.get('eventDescription', '')[:100]}\"\n",
        "        search_results = []\n",
        "        for result in search(search_query, num_results=3):\n",
        "            search_results.append(result)\n",
        "\n",
        "        # Add scores from search results\n",
        "        search_text = ' '.join(search_results).lower()\n",
        "        for category, keywords in category_keywords.items():\n",
        "            search_score = sum(1 for keyword in keywords if keyword.lower() in search_text)\n",
        "            category_scores[category] += search_score * 0.5  # Weight search results less than direct matches\n",
        "\n",
        "        # Return the category with the highest score\n",
        "        max_score = max(category_scores.values())\n",
        "        if max_score > 0:\n",
        "            return max(category_scores.items(), key=lambda x: x[1])[0]\n",
        "        return 'Otros'  # Default category if no clear match\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in category determination: {e}\")\n",
        "        return 'Otros'\n",
        "\n",
        "def enrich_event_data(events_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Enrich events dataframe with coordinates and categories.\n",
        "    \"\"\"\n",
        "    enriched_df = events_df.copy()\n",
        "    enriched_df['latlng'] = None\n",
        "    enriched_df['eventCategory'] = None\n",
        "\n",
        "    for idx, row in enriched_df.iterrows():\n",
        "        try:\n",
        "            # Fetch the event page again to get coordinates\n",
        "            response = requests.get(row['event_url'])\n",
        "            if response.status_code == 200:\n",
        "                # Extract coordinates\n",
        "                coords = extract_coordinates(response.text)\n",
        "                if coords:\n",
        "                    enriched_df.at[idx, 'latlng'] = str(coords)\n",
        "\n",
        "                # Determine category\n",
        "                event_data = row.to_dict()\n",
        "                category = determine_event_category(event_data)\n",
        "                enriched_df.at[idx, 'eventCategory'] = category\n",
        "\n",
        "            print(f\"Enriched data for event: {row['eventName']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error enriching data for event {row['eventName']}: {e}\")\n",
        "\n",
        "        # Add delay between requests\n",
        "        time.sleep(2)\n",
        "\n",
        "    return enriched_df\n",
        "\n",
        "def save_enriched_data(df: pd.DataFrame, output_path: str):\n",
        "    \"\"\"Save enriched data to both CSV and JSON formats.\"\"\"\n",
        "    # Save CSV\n",
        "    df.to_csv(f\"{output_path}_enriched.csv\", index=False)\n",
        "\n",
        "    # Save JSON\n",
        "    df.to_json(f\"{output_path}_enriched.json\", orient='records', indent=2)\n",
        "\n",
        "def main():\n",
        "    # Load the original scraped data\n",
        "    input_path = \"output/ticketea_events_latest\"  # Adjust path as needed\n",
        "    df = pd.read_csv(f\"{input_path}.csv\")\n",
        "\n",
        "    # Enrich the data\n",
        "    print(\"Starting data enrichment process...\")\n",
        "    enriched_df = enrich_event_data(df)\n",
        "\n",
        "    # Save enriched data\n",
        "    save_enriched_data(enriched_df, input_path)\n",
        "    print(\"Data enrichment completed and saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "Nz-71BoxeJ13",
        "outputId": "79f11d56-ed7e-404d-853d-bc662d271113",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data enrichment process...\n",
            "Error in category determination: 'eventName'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'eventName'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eventName'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2884c38bd5b7>\u001b[0m in \u001b[0;36menrich_event_data\u001b[0;34m(events_df)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Enriched data for event: {row['eventName']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eventName'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eventName'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2884c38bd5b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-2884c38bd5b7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;31m# Enrich the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting data enrichment process...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0menriched_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menrich_event_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Save enriched data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2884c38bd5b7>\u001b[0m in \u001b[0;36menrich_event_data\u001b[0;34m(events_df)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error enriching data for event {row['eventName']}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Add delay between requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'eventName'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asukeai"
      ],
      "metadata": {
        "id": "hhS809Jd9rjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install webdriver_manager\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import unquote\n",
        "import uuid\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "def extract_google_maps_coordinates(url):\n",
        "    \"\"\"\n",
        "    Extract coordinates from Google Maps URL by following redirects and parsing the final URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, allow_redirects=True)\n",
        "        final_url = response.url\n",
        "\n",
        "        patterns = [\n",
        "            r'@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)',  # Pattern for @lat,lng\n",
        "            r'll=(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)',  # Pattern for ll=lat,lng\n",
        "            r'q=(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)'    # Pattern for q=lat,lng\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, final_url)\n",
        "            if match:\n",
        "                return {\n",
        "                    \"latitude\": float(match.group(1)),\n",
        "                    \"longitude\": float(match.group(2))\n",
        "                }\n",
        "\n",
        "        print(f\"Could not extract coordinates from URL: {final_url}\")\n",
        "        return {\n",
        "            \"latitude\": -25.2867,\n",
        "            \"longitude\": -57.3333\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting coordinates: {str(e)}\")\n",
        "        return {\n",
        "            \"latitude\": -25.2867,\n",
        "            \"longitude\": -57.3333\n",
        "        }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text content.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    return \" \".join(text.strip().split())\n",
        "\n",
        "def extract_price(price_text):\n",
        "    \"\"\"Extract numerical price from text.\"\"\"\n",
        "    if not price_text:\n",
        "        return \"0\"\n",
        "    numbers = re.findall(r'\\d+', price_text.replace('.', ''))\n",
        "    return numbers[0] if numbers else \"0\"\n",
        "\n",
        "def parse_event_block(block):\n",
        "    \"\"\"Parse a single event block and return structured data.\"\"\"\n",
        "    soup = BeautifulSoup(block, 'html.parser')\n",
        "\n",
        "    event = {\n",
        "        \"title\": \"\",\n",
        "        \"image_url\": \"\",\n",
        "        \"image_alt\": \"\",\n",
        "        \"image_title\": \"\",\n",
        "        \"event_url\": \"\",\n",
        "        \"event_hours\": \"\",\n",
        "        \"event_location\": \"\",\n",
        "        \"event_address\": \"\",\n",
        "        \"additional_details\": \"\",\n",
        "        \"coordinates\": {\"latitude\": 0, \"longitude\": 0},\n",
        "        \"meta\": {\n",
        "            \"id\": str(uuid.uuid4()),\n",
        "            \"category\": \"\",\n",
        "            \"price\": \"0\",\n",
        "            \"currency\": \"PYG\",\n",
        "            \"brand\": \"Notion Events\",\n",
        "            \"availability\": \"in stock\",\n",
        "            \"condition\": \"new\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Extract title\n",
        "    title_div = soup.find('div', attrs={'data-content-editable-leaf': 'true'})\n",
        "    if title_div:\n",
        "        event[\"title\"] = clean_text(title_div.text)\n",
        "        event[\"image_alt\"] = event[\"title\"]\n",
        "        event[\"image_title\"] = event[\"title\"]\n",
        "\n",
        "    # Extract image\n",
        "    img_tag = soup.find('img')\n",
        "    if img_tag and 'src' in img_tag.attrs:\n",
        "        src = img_tag['src']\n",
        "        if src.startswith('/image/'):\n",
        "            src = unquote(src[7:].split('?')[0])\n",
        "        event[\"image_url\"] = src\n",
        "\n",
        "    # Extract date and time\n",
        "    date_div = soup.find('div', text=re.compile(r'\\w+\\s+\\d+,\\s+\\d{4}'))\n",
        "    time_div = soup.find('div', text=re.compile(r'\\d{1,2}:\\d{2}\\s*Hs\\.'))\n",
        "\n",
        "    if date_div and time_div:\n",
        "        event[\"event_hours\"] = f\"{clean_text(date_div.text)}, {clean_text(time_div.text)}\"\n",
        "\n",
        "    # Extract location and coordinates\n",
        "    location_link = soup.find('a', href=re.compile(r'maps\\.google\\.com|goo\\.gl'))\n",
        "    if location_link:\n",
        "        event[\"event_location\"] = clean_text(location_link.text)\n",
        "        event[\"event_address\"] = event[\"event_location\"]\n",
        "        event[\"coordinates\"] = extract_google_maps_coordinates(location_link['href'])\n",
        "\n",
        "    # Extract category\n",
        "    category_span = soup.find('span', text=re.compile(r'Música|Teatro|Arte|Cine|Capacitación'))\n",
        "    if category_span:\n",
        "        event[\"meta\"][\"category\"] = clean_text(category_span.text)\n",
        "\n",
        "    # Extract price\n",
        "    price_span = soup.find('span', text=re.compile(r'Gs\\.\\s*[\\d\\.]+'))\n",
        "    if price_span:\n",
        "        event[\"meta\"][\"price\"] = extract_price(price_span.text)\n",
        "\n",
        "    # Extract description\n",
        "    description_spans = soup.find_all('span', recursive=True)\n",
        "    descriptions = []\n",
        "    for span in description_spans:\n",
        "        if len(clean_text(span.text)) > 30:  # Assume longer texts are descriptions\n",
        "            descriptions.append(clean_text(span.text))\n",
        "    if descriptions:\n",
        "        event[\"additional_details\"] = \" | \".join(descriptions)\n",
        "\n",
        "    return event\n",
        "\n",
        "def parse_notion_events(html_content):\n",
        "    \"\"\"Parse all event blocks from the Notion page.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    event_blocks = soup.find_all('div', attrs={'data-block-id': True, 'class': 'notion-selectable notion-page-block notion-collection-item'})\n",
        "\n",
        "    events = []\n",
        "    for block in event_blocks:\n",
        "        try:\n",
        "            event = parse_event_block(str(block))\n",
        "            events.append(event)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing block: {e}\")\n",
        "            continue\n",
        "\n",
        "    return events\n",
        "\n",
        "# Setup Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Setup the driver\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "\n",
        "# Load the page\n",
        "url = \"https://asukeai.notion.site/f3e4701380844f29a8775995fd9f9747?v=3c1dfc5a47d346d5a65ccc8c596c843c\"\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the content to load\n",
        "wait = WebDriverWait(driver, 20)\n",
        "try:\n",
        "    # Wait for events to load\n",
        "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"notion-collection-item\")))\n",
        "    # Give additional time for everything to render\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Get the page source after JavaScript has rendered\n",
        "    html_content = driver.page_source\n",
        "\n",
        "    # Parse events\n",
        "    events = parse_notion_events(html_content)\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open('events.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(events, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"Events have been saved to events.json\")\n",
        "    print(f\"Total events extracted: {len(events)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading page: {e}\")\n",
        "\n",
        "finally:\n",
        "    driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "84nYYvDD9qSZ",
        "outputId": "c0451a35-fd8b-4d66-f08e-33243496bb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.27.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [61.9 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,199 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,631 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,513 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,554 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,854 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,652 kB]\n",
            "Fetched 19.6 MB in 3s (7,190 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 30.2 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.66.1+22.04 [27.6 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.2 MB in 2s (13.1 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123840 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.66.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.66.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.66.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124069 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.12.14)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "WebDriverException",
          "evalue": "Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/chromium-browser is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x59c5f8f6e4e3 <unknown>\n#1 0x59c5f8c9dc76 <unknown>\n#2 0x59c5f8cc6d78 <unknown>\n#3 0x59c5f8cc3029 <unknown>\n#4 0x59c5f8d01ccc <unknown>\n#5 0x59c5f8d0147f <unknown>\n#6 0x59c5f8cf8de3 <unknown>\n#7 0x59c5f8cce2dd <unknown>\n#8 0x59c5f8ccf34e <unknown>\n#9 0x59c5f8f2e3e4 <unknown>\n#10 0x59c5f8f323d7 <unknown>\n#11 0x59c5f8f3cb20 <unknown>\n#12 0x59c5f8f33023 <unknown>\n#13 0x59c5f8f011aa <unknown>\n#14 0x59c5f8f576b8 <unknown>\n#15 0x59c5f8f57847 <unknown>\n#16 0x59c5f8f67243 <unknown>\n#17 0x7aca406acac3 <unknown>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cd101b6ae54b>\u001b[0m in \u001b[0;36m<cell line: 170>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;31m# Setup the driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchrome_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;31m# Load the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mvendor_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fedcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFedCM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mWebDriverException\u001b[0m: Message: unknown error: Chrome failed to start: exited abnormally.\n  (unknown error: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /usr/bin/chromium-browser is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x59c5f8f6e4e3 <unknown>\n#1 0x59c5f8c9dc76 <unknown>\n#2 0x59c5f8cc6d78 <unknown>\n#3 0x59c5f8cc3029 <unknown>\n#4 0x59c5f8d01ccc <unknown>\n#5 0x59c5f8d0147f <unknown>\n#6 0x59c5f8cf8de3 <unknown>\n#7 0x59c5f8cce2dd <unknown>\n#8 0x59c5f8ccf34e <unknown>\n#9 0x59c5f8f2e3e4 <unknown>\n#10 0x59c5f8f323d7 <unknown>\n#11 0x59c5f8f3cb20 <unknown>\n#12 0x59c5f8f33023 <unknown>\n#13 0x59c5f8f011aa <unknown>\n#14 0x59c5f8f576b8 <unknown>\n#15 0x59c5f8f57847 <unknown>\n#16 0x59c5f8f67243 <unknown>\n#17 0x7aca406acac3 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuti Scraping\n"
      ],
      "metadata": {
        "id": "TWBgBtVZSuEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def extract_meta_property(soup, property_name):\n",
        "    \"\"\"Helper function to extract meta tag properties.\"\"\"\n",
        "    meta_tag = soup.find('meta', property=property_name)\n",
        "    return meta_tag.get('content') if meta_tag else None\n",
        "\n",
        "def fetch_event_links(base_url=\"https://tuti.com.py\"):\n",
        "    \"\"\"Extract all event links from the main page.\"\"\"\n",
        "    print(f\"\\nFetching events from: {base_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        print(f\"Status code for main page: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching main page: {e}\")\n",
        "        return []\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch main page. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    return extract_events(response.content)\n",
        "\n",
        "def fetch_event_details(event_url, basic_info):\n",
        "    \"\"\"Fetch detailed information for a single event.\"\"\"\n",
        "    print(f\"\\nFetching details for event: {event_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(event_url)\n",
        "        print(f\"Status code: {response.status_code}\")\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Initialize details dictionary\n",
        "        details = {}\n",
        "\n",
        "        # Extract map coordinates if available\n",
        "        map_iframe = soup.find('iframe', {'src': lambda x: x and 'maps.google.com/maps' in x})\n",
        "        if map_iframe:\n",
        "            src = map_iframe.get('src', '')\n",
        "            coords_match = re.search(r'q=([-\\d\\.]+),([-\\d\\.]+)', src)\n",
        "            if coords_match:\n",
        "                details['coordinates'] = {\n",
        "                    'latitude': coords_match.group(1),\n",
        "                    'longitude': coords_match.group(2)\n",
        "                }\n",
        "\n",
        "        # Extract meta properties\n",
        "        basic_info['meta'] = {\n",
        "            'id': extract_meta_property(soup, 'og:id'),\n",
        "            'category': extract_meta_property(soup, 'og:product:category'),\n",
        "            'price': extract_meta_property(soup, 'og:product:price:amount'),\n",
        "            'currency': extract_meta_property(soup, 'og:product:price:currency'),\n",
        "            'brand': extract_meta_property(soup, 'og:product:brand'),\n",
        "            'availability': extract_meta_property(soup, 'og:product:availability'),\n",
        "            'description': extract_meta_property(soup, 'og:description')\n",
        "        }\n",
        "\n",
        "        # Extract event information\n",
        "        info_div = soup.find('div', {'class': 'informacion'})\n",
        "        if info_div:\n",
        "            event_info = []\n",
        "            info_header = info_div.find('h4')\n",
        "            if info_header and info_header.text.strip() == 'Información del evento':\n",
        "                paragraphs = info_div.find_all('p')\n",
        "                for p in paragraphs:\n",
        "                    text = p.get_text(strip=True)\n",
        "                    if text:\n",
        "                        event_info.append(text)\n",
        "            if event_info:\n",
        "                details['event_information'] = event_info\n",
        "\n",
        "        # Extract sectors and prices\n",
        "        sectors_div = soup.find('section', {'class': 'sectores_disponibles'})\n",
        "        if sectors_div:\n",
        "            sectors = []\n",
        "            sector_rows = sectors_div.find_all('div', {'class': 'caja_sectores_fila'})\n",
        "            for row in sector_rows:\n",
        "                sector_name = row.find('div', {'class': 'caja_sectores_nombre'})\n",
        "                sector_price = row.find('div', {'class': 'caja_sectores_precio'})\n",
        "                if sector_name and sector_price:\n",
        "                    sectors.append({\n",
        "                        'name': sector_name.text.strip(),\n",
        "                        'price': sector_price.text.strip()\n",
        "                    })\n",
        "            if sectors:\n",
        "                details['sectors'] = sectors\n",
        "\n",
        "        # Get venue information\n",
        "        location_details = soup.find(\"div\", class_=\"info-event\")\n",
        "        if location_details:\n",
        "            details['venue'] = location_details.get_text(strip=True)\n",
        "\n",
        "        basic_info['details'] = details\n",
        "        return basic_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching event details: {e}\")\n",
        "        return basic_info\n",
        "\n",
        "def extract_events(html):\n",
        "    \"\"\"Extract events from the main page HTML\"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    events = []\n",
        "\n",
        "    event_blocks = soup.find_all(\"div\", class_=\"un_evento\")\n",
        "\n",
        "    for block in event_blocks:\n",
        "        try:\n",
        "            # Extract basic info\n",
        "            link = block.find(\"a\", href=True)['href']\n",
        "            title = block.find(\"h2\", class_=\"un_evento_title\").text.strip()\n",
        "\n",
        "            info_spans = block.find_all(\"span\")\n",
        "            date = info_spans[0].text.strip() if info_spans else \"No date\"\n",
        "            location = info_spans[1].text.strip() if len(info_spans) > 1 else \"No location\"\n",
        "\n",
        "            price_btn = block.find(\"div\", class_=\"un_evento_info_bottom\").find(\"a\", class_=\"btn-primary\")\n",
        "            if price_btn:\n",
        "                price_text = price_btn.text.strip()\n",
        "                # Remove \"Comprá desde\" and extract just the number\n",
        "                price = price_text.replace(\"Comprá desde\", \"\").replace(\"PYG\", \"\").strip()\n",
        "            else:\n",
        "                price = \"No price\"\n",
        "\n",
        "            img_tag = block.find(\"img\")\n",
        "            img_url = img_tag.get('src', '') if img_tag else ''\n",
        "\n",
        "            # Check if the link already has the full URL\n",
        "            if link.startswith('http'):\n",
        "                full_url = link\n",
        "            else:\n",
        "                full_url = urljoin(\"https://tuti.com.py\", link)\n",
        "\n",
        "            event_info = {\n",
        "                \"title\": title,\n",
        "                \"url\": full_url,\n",
        "                \"date\": date,\n",
        "                \"location\": location,\n",
        "                \"price\": price,\n",
        "                \"image_url\": img_url\n",
        "            }\n",
        "\n",
        "            events.append(event_info)\n",
        "            print(f\"Found event: {title}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing event block: {e}\")\n",
        "            continue\n",
        "\n",
        "    return events\n",
        "\n",
        "def write_to_files(events, timestamp):\n",
        "    \"\"\"Write scraped data to both CSV and JSON files.\"\"\"\n",
        "    if not events:\n",
        "        print(\"No event data available to write.\")\n",
        "        return\n",
        "\n",
        "    output_dir = \"output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Write JSON\n",
        "    json_path = os.path.join(output_dir, f\"tuti_events_{timestamp}.json\")\n",
        "    with open(json_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(events, file, ensure_ascii=False, indent=2)\n",
        "    print(f\"JSON file saved: {json_path}\")\n",
        "\n",
        "    # Write CSV - updated to include new fields\n",
        "    csv_path = os.path.join(output_dir, f\"tuti_events_{timestamp}.csv\")\n",
        "    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        fieldnames = [\n",
        "            'title', 'url', 'date', 'location', 'price', 'image_url',\n",
        "            'meta_id', 'meta_category', 'meta_price', 'meta_currency',\n",
        "            'meta_brand', 'meta_availability', 'description', 'venue',\n",
        "            'latitude', 'longitude'\n",
        "        ]\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for event in events:\n",
        "            coordinates = event.get('details', {}).get('coordinates', {})\n",
        "            flat_event = {\n",
        "                'title': event.get('title', ''),\n",
        "                'url': event.get('url', ''),\n",
        "                'date': event.get('date', ''),\n",
        "                'location': event.get('location', ''),\n",
        "                'price': event.get('price', ''),\n",
        "                'image_url': event.get('image_url', ''),\n",
        "                'meta_id': event.get('meta', {}).get('id', ''),\n",
        "                'meta_category': event.get('meta', {}).get('category', ''),\n",
        "                'meta_price': event.get('meta', {}).get('price', ''),\n",
        "                'meta_currency': event.get('meta', {}).get('currency', ''),\n",
        "                'meta_brand': event.get('meta', {}).get('brand', ''),\n",
        "                'meta_availability': event.get('meta', {}).get('availability', ''),\n",
        "                'description': event.get('meta', {}).get('description', ''),\n",
        "                'venue': event.get('details', {}).get('venue', ''),\n",
        "                'latitude': coordinates.get('latitude', ''),\n",
        "                'longitude': coordinates.get('longitude', '')\n",
        "            }\n",
        "            writer.writerow(flat_event)\n",
        "\n",
        "    print(f\"CSV file saved: {csv_path}\")\n",
        "\n",
        "def main():\n",
        "    print(\"\\n=== Starting TUTI event scraping process... ===\\n\")\n",
        "\n",
        "    # Get all event links from main page\n",
        "    events = fetch_event_links()\n",
        "    print(f\"\\nFound {len(events)} events on main page.\")\n",
        "\n",
        "    # Get detailed information for each event\n",
        "    for event in events:\n",
        "        print(f\"\\nScraping details for: {event['title']}\")\n",
        "        event_details = fetch_event_details(event['url'], event)\n",
        "        if event_details:\n",
        "            print(f\"Successfully scraped details for: {event['title']}\")\n",
        "\n",
        "        # Sleep between requests to avoid overloading the server\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Generate timestamp and save data\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    write_to_files(events, timestamp)\n",
        "\n",
        "    print(f\"\\n=== Scraping completed. Total events processed: {len(events)} ===\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZeM3IBvgNkK",
        "outputId": "bee7859c-54da-43fc-b0d8-a5e770de9f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting TUTI event scraping process... ===\n",
            "\n",
            "\n",
            "Fetching events from: https://tuti.com.py\n",
            "Status code for main page: 200\n",
            "\n",
            "Found 0 events on main page.\n",
            "No event data available to write.\n",
            "\n",
            "=== Scraping completed. Total events processed: 0 ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import re\n",
        "from typing import Dict, Optional, Tuple, List\n",
        "\n",
        "def parse_spanish_date(date_str: str) -> Optional[str]:\n",
        "    \"\"\"Parse Spanish date string to ISO format timestamp.\"\"\"\n",
        "    try:\n",
        "        spanish_months = {\n",
        "            'enero': 1, 'febrero': 2, 'marzo': 3, 'abril': 4,\n",
        "            'mayo': 5, 'junio': 6, 'julio': 7, 'agosto': 8,\n",
        "            'septiembre': 9, 'octubre': 10, 'noviembre': 11, 'diciembre': 12\n",
        "        }\n",
        "\n",
        "        # Remove day name and split at comma\n",
        "        main_parts = date_str.split(',')[0].split()\n",
        "        time_part = date_str.split(',')[1].strip()\n",
        "\n",
        "        # Extract day and month\n",
        "        day = int([x for x in main_parts if x.isdigit()][0])\n",
        "        month_str = next(x.lower() for x in main_parts if x.lower() in spanish_months)\n",
        "        month = spanish_months[month_str]\n",
        "\n",
        "        # Extract time\n",
        "        time_match = re.search(r'(\\d{1,2}):(\\d{2})', time_part)\n",
        "        if time_match:\n",
        "            hour = int(time_match.group(1))\n",
        "            minute = int(time_match.group(2))\n",
        "\n",
        "            # Get current year or next year if the date has passed\n",
        "            current_date = datetime.now()\n",
        "            year = current_date.year\n",
        "            event_date = datetime(year, month, day, hour, minute)\n",
        "\n",
        "            # If the date has passed, use next year\n",
        "            if event_date < current_date:\n",
        "                event_date = datetime(year + 1, month, day, hour, minute)\n",
        "\n",
        "            return event_date.isoformat()\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date {date_str}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_coordinates_from_schema(html_content: str) -> Optional[Tuple[float, float]]:\n",
        "    \"\"\"Extract coordinates from schema.org metadata.\"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        script_tags = soup.find_all('script', type='application/ld+json')\n",
        "\n",
        "        for script in script_tags:\n",
        "            try:\n",
        "                data = json.loads(script.string)\n",
        "                if isinstance(data, dict) and data.get('@type') == 'Event':\n",
        "                    location = data.get('location', {})\n",
        "                    if location.get('geo'):\n",
        "                        lat = float(location['geo'].get('latitude'))\n",
        "                        lng = float(location['geo'].get('longitude'))\n",
        "                        return (lat, lng)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting coordinates from schema: {e}\")\n",
        "        return None\n",
        "\n",
        "def format_coordinates(coords: Tuple[float, float]) -> List[str]:\n",
        "    \"\"\"Format coordinates as [latitude° S/N, longitude° E/W].\"\"\"\n",
        "    if not coords:\n",
        "        return []\n",
        "\n",
        "    lat, lng = coords\n",
        "    lat_hemisphere = 'S' if lat < 0 else 'N'\n",
        "    lng_hemisphere = 'W' if lng < 0 else 'E'\n",
        "\n",
        "    lat_str = f\"{abs(lat)}° {lat_hemisphere}\"\n",
        "    lng_str = f\"{abs(lng)}° {lng_hemisphere}\"\n",
        "\n",
        "    return [lat_str, lng_str]\n",
        "\n",
        "def extract_price_from_text(text: str) -> Optional[int]:\n",
        "    \"\"\"Extract the lowest price from text looking for Gs. pattern.\"\"\"\n",
        "    try:\n",
        "        price_pattern = r'(?:Gs\\.?\\s*(\\d{1,3}(?:\\.\\d{3})*)|(\\d{1,3}(?:\\.\\d{3})*)\\s*Gs\\.?)'\n",
        "        matches = re.findall(price_pattern, text)\n",
        "\n",
        "        if matches:\n",
        "            prices = []\n",
        "            for match in matches:\n",
        "                price_str = match[0] if match[0] else match[1]\n",
        "                price = int(price_str.replace('.', ''))\n",
        "                prices.append(price)\n",
        "            return min(prices) if prices else None\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting price: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_event_title(title: str) -> str:\n",
        "    \"\"\"Clean event title by removing website suffix and 'Entradas para' prefix.\"\"\"\n",
        "    title = re.sub(r'\\s*-\\s*Ticketea\\.com\\.py$', '', title)\n",
        "    title = re.sub(r'^Entradas\\s+para\\s+', '', title)\n",
        "    return title.strip()\n",
        "\n",
        "def fetch_event_links():\n",
        "    \"\"\"Fetch all event links from the main page.\"\"\"\n",
        "    base_url = \"https://ticketea.com.py\"\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch main page. Status code: {response.status_code}\")\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        event_links = [f\"{base_url}{link['href']}\" for link in soup.find_all('a', href=True)\n",
        "                      if link['href'].startswith(\"/events/\")]\n",
        "\n",
        "        unique_links = list(set(event_links))\n",
        "        print(f\"Found {len(unique_links)} unique event links\")\n",
        "        return unique_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching main page: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_event_details(event_url: str) -> Optional[Dict]:\n",
        "    \"\"\"Fetch and parse event details from the event page.\"\"\"\n",
        "    print(f\"\\nFetching details for event: {event_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(event_url)\n",
        "        if response.status_code != 200:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title = soup.find('meta', {'property': 'og:title'})\n",
        "        title = clean_event_title(title['content']) if title else \"\"\n",
        "\n",
        "        image = soup.find('meta', {'property': 'og:image'})\n",
        "        image_url = image['content'] if image else \"\"\n",
        "\n",
        "        description_div = soup.find('div', {'class': 'description-content'})\n",
        "        if description_div:\n",
        "            description_text = description_div.get_text()\n",
        "            price = extract_price_from_text(description_text)\n",
        "        else:\n",
        "            description = soup.find('meta', {'property': 'og:description'})\n",
        "            description_text = description['content'] if description else \"\"\n",
        "            price = 0\n",
        "\n",
        "        coords = extract_coordinates_from_schema(response.text)\n",
        "        latlong = format_coordinates(coords) if coords else []\n",
        "\n",
        "        date_time_elem = soup.find('p', {'class': 'size-md m-b-0 line-height-20'})\n",
        "        event_datetime = date_time_elem.text.strip() if date_time_elem else None\n",
        "        parsed_datetime = parse_spanish_date(event_datetime) if event_datetime else datetime.now().isoformat()\n",
        "\n",
        "        address_elem = soup.find('meta', {'property': 'og:description'})\n",
        "        if address_elem:\n",
        "            address_parts = address_elem['content'].split(',')\n",
        "            street = address_parts[0].strip() if len(address_parts) > 0 else \"\"\n",
        "            city = address_parts[1].strip() if len(address_parts) > 1 else \"\"\n",
        "        else:\n",
        "            street, city = \"\", \"\"\n",
        "\n",
        "        event_data = {\n",
        "            \"eventId\": event_url.split('/')[-1],\n",
        "            \"eventName\": title,\n",
        "            \"eventPhoto\": image_url,\n",
        "            \"createdTime\": datetime.now().isoformat(),\n",
        "            \"eventStartTime\": parsed_datetime,\n",
        "            \"eventEndTime\": parsed_datetime,\n",
        "            \"priceTicket\": int(price) if price else 0,\n",
        "            \"registeredUsers\": \"\",\n",
        "            \"street\": street,\n",
        "            \"city\": city,\n",
        "            \"zipcode\": \"\",\n",
        "            \"country\": \"PY\",\n",
        "            \"uid\": \"/users/rBU6BGqtw1QAMeXOxUEktFtfVan1\",\n",
        "            \"eventDescription\": description_text,\n",
        "            \"eventCategory\": None,\n",
        "            \"latlong\": latlong,\n",
        "            \"eventPrice\": int(price) if price else 0,\n",
        "            \"name\": city\n",
        "        }\n",
        "\n",
        "        event_data['eventCategory'] = determine_event_category(event_data)\n",
        "\n",
        "        print(f\"Successfully processed: {title}\")\n",
        "        print(f\"Date: {parsed_datetime}\")\n",
        "        print(f\"Price: {price}\")\n",
        "        print(f\"Coordinates: {latlong}\")\n",
        "\n",
        "        return event_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing event {event_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def determine_event_category(event_data: Dict) -> str:\n",
        "    \"\"\"Determine event category based on predefined local categories.\"\"\"\n",
        "    category_keywords = {\n",
        "        'Capacitaciones': ['capacitación', 'taller', 'curso', 'workshop'],\n",
        "        'Música': ['música', 'concierto', 'festival', 'show', 'banda'],\n",
        "        'Teatro': ['teatro', 'obra', 'drama'],\n",
        "        'Deportes': ['deporte', 'partido', 'torneo']\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        text = f\"{event_data['eventName']} {event_data.get('eventDescription', '')}\".lower()\n",
        "        scores = {cat: sum(1 for kw in kws if kw.lower() in text)\n",
        "                 for cat, kws in category_keywords.items()}\n",
        "\n",
        "        if scores:\n",
        "            return max(scores.items(), key=lambda x: x[1])[0]\n",
        "        return 'Otros'\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error determining category: {e}\")\n",
        "        return 'Otros'\n",
        "\n",
        "def save_results(events, timestamp):\n",
        "    \"\"\"Save results in a format compatible with FlutterFlow import.\"\"\"\n",
        "    output_dir = \"output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    csv_path = os.path.join(output_dir, f\"ticketea_events_{timestamp}.csv\")\n",
        "\n",
        "    # Clean and standardize the data for CSV\n",
        "    csv_events = []\n",
        "    for event in events:\n",
        "        csv_event = event.copy()\n",
        "        # Convert latlong to string if it's a list\n",
        "        if isinstance(csv_event['latlong'], list):\n",
        "            csv_event['latlong'] = ', '.join(csv_event['latlong'])\n",
        "        csv_events.append(csv_event)\n",
        "\n",
        "    df = pd.DataFrame(csv_events)\n",
        "    df.to_csv(csv_path,\n",
        "              index=False,\n",
        "              encoding='utf-8',\n",
        "              quoting=csv.QUOTE_MINIMAL,\n",
        "              na_rep='')\n",
        "\n",
        "    return csv_path\n",
        "\n",
        "def main():\n",
        "    print(\"Starting scraping process...\")\n",
        "    event_links = fetch_event_links()\n",
        "    events = []\n",
        "\n",
        "    for event_url in event_links:\n",
        "        try:\n",
        "            event_data = fetch_event_details(event_url)\n",
        "            if event_data:\n",
        "                events.append(event_data)\n",
        "                print(f\"Successfully added event: {event_data['eventName']}\")\n",
        "            time.sleep(2)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing event {event_url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if events:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        csv_path = save_results(events, timestamp)\n",
        "\n",
        "        print(f\"\\nScraping completed!\")\n",
        "        print(f\"Total events processed: {len(events)}\")\n",
        "        print(f\"File saved: {csv_path}\")\n",
        "    else:\n",
        "        print(\"No events were successfully processed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EcmcM90wDmK",
        "outputId": "00a8f2b5-7148-4399-e6c0-2bc852cf0721"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scraping process...\n",
            "Found 35 unique event links\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/juli-hernandez-en-asuncion\n",
            "Successfully processed: JULI HERNANDEZ EN ASUNCION\n",
            "Date: 2025-07-11T22:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2908566° S', '57.6177523° W']\n",
            "Successfully added event: JULI HERNANDEZ EN ASUNCION\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/pisada-indie\n",
            "Successfully processed: PISADA INDIE\n",
            "Date: 2025-07-04T20:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2872649° S', '57.62565° W']\n",
            "Successfully added event: PISADA INDIE\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/barca-academy-campus-minga-guazu-2025\n",
            "Successfully processed: BARÇA ACADEMY CAMPUS MINGA GUAZÚ 2025\n",
            "Date: 2025-07-21T08:30:00\n",
            "Price: 3120000\n",
            "Coordinates: ['25.4871801° S', '54.782938° W']\n",
            "Successfully added event: BARÇA ACADEMY CAMPUS MINGA GUAZÚ 2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/bootcamp-los-secretos-detras-de-la-magia\n",
            "Successfully processed: BOOTCAMP – LOS SECRETOS DETRAS DE LA MAGIA\n",
            "Date: 2025-07-02T08:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2839672° S', '57.5646901° W']\n",
            "Successfully added event: BOOTCAMP – LOS SECRETOS DETRAS DE LA MAGIA\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/eyesight-en-vivo\n",
            "Successfully processed: EYESIGHT EN VIVO\n",
            "Date: 2025-07-05T22:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2872649° S', '57.62565° W']\n",
            "Successfully added event: EYESIGHT EN VIVO\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/ellen-allien-ologram\n",
            "Successfully processed: ELLEN ALLIEN - OLOGRAM\n",
            "Date: 2025-06-14T23:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2744463° S', '57.6445734° W']\n",
            "Successfully added event: ELLEN ALLIEN - OLOGRAM\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/la-vela-puerca-en-paraguay\n",
            "Successfully processed: LA VELA PUERCA EN PARAGUAY\n",
            "Date: 2025-08-02T21:00:00\n",
            "Price: 250000\n",
            "Coordinates: ['25.2744463° S', '57.6445734° W']\n",
            "Successfully added event: LA VELA PUERCA EN PARAGUAY\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/sunshine-w-mihigh\n",
            "Successfully processed: SUNSHINE W/ MIHIGH\n",
            "Date: 2025-06-08T05:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2841103° S', '57.6316216° W']\n",
            "Successfully added event: SUNSHINE W/ MIHIGH\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/gustavisimo-25-anos\n",
            "Successfully processed: GUSTAVISIMO - 25 AÑOS\n",
            "Date: 2025-06-06T21:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2893291° S', '57.636354° W']\n",
            "Successfully added event: GUSTAVISIMO - 25 AÑOS\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/mamma-mia-2025-05-31-21-00-00-0400\n",
            "Successfully processed: MAMMA MIA\n",
            "Date: 2025-06-28T21:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2893291° S', '57.636354° W']\n",
            "Successfully added event: MAMMA MIA\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/sanjuanazo\n",
            "Successfully processed: SANJUANAZO\n",
            "Date: 2025-06-21T17:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.1780731° S', '57.4833457° W']\n",
            "Successfully added event: SANJUANAZO\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/the-messengers-conference-con-lucas-magnin-y-kike-pavon\n",
            "Successfully processed: THE MESSENGERS CONFERENCE CON LUCAS MAGNIN Y KIKE PAVON\n",
            "Date: 2025-06-28T12:00:00\n",
            "Price: 100000\n",
            "Coordinates: ['25.2846953° S', '57.5633369° W']\n",
            "Successfully added event: THE MESSENGERS CONFERENCE CON LUCAS MAGNIN Y KIKE PAVON\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/un-mentes-expertas-de-victor-kuppers-vivir-con-actitud\n",
            "Successfully processed: UN MENTES EXPERTAS DE VICTOR KÜPPERS – VIVIR CON ACTITUD\n",
            "Date: 2025-06-02T20:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2787883° S', '57.5752972° W']\n",
            "Successfully added event: UN MENTES EXPERTAS DE VICTOR KÜPPERS – VIVIR CON ACTITUD\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/insideout-week\n",
            "Successfully processed: INSIDEOUT WEEK\n",
            "Date: 2025-06-24T08:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2816584° S', '57.5653513° W']\n",
            "Successfully added event: INSIDEOUT WEEK\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/miranda-en-paraguay\n",
            "Successfully processed: MIRANDA! EN PARAGUAY\n",
            "Date: 2025-08-30T21:00:00\n",
            "Price: 187500\n",
            "Coordinates: ['25.2744463° S', '57.6445734° W']\n",
            "Successfully added event: MIRANDA! EN PARAGUAY\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/expo-uruguay-wine-2025\n",
            "Successfully processed: EXPO URUGUAY WINE 2025\n",
            "Date: 2025-06-18T18:30:00\n",
            "Price: None\n",
            "Coordinates: ['25.2839672° S', '57.5646901° W']\n",
            "Successfully added event: EXPO URUGUAY WINE 2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/green-day-en-paraguay\n",
            "Successfully processed: GREEN DAY EN PARAGUAY\n",
            "Date: 2025-05-29T17:29:28.297751\n",
            "Price: 0\n",
            "Coordinates: ['25.3165692° S', '57.5883495° W']\n",
            "Successfully added event: GREEN DAY EN PARAGUAY\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/ana-castela\n",
            "Successfully processed: ANA CASTELA\n",
            "Date: 2025-11-14T21:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.4959427° S', '54.6648416° W']\n",
            "Successfully added event: ANA CASTELA\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/mora-en-paraguay\n",
            "Successfully processed: MORA - LO MISMO DE LA OTRA VEZ TOUR\n",
            "Date: 2025-05-29T17:29:33.661927\n",
            "Price: 0\n",
            "Coordinates: ['25.2744463° S', '57.6445734° W']\n",
            "Successfully added event: MORA - LO MISMO DE LA OTRA VEZ TOUR\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/fuckup-nights\n",
            "Successfully processed: FUCKUP NIGHTS\n",
            "Date: 2025-06-04T19:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2857084° S', '57.5772497° W']\n",
            "Successfully added event: FUCKUP NIGHTS\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/chayanne-bailemos-otra-vez-tour-2025\n",
            "Successfully processed: CHAYANNE - BAILEMOS OTRA VEZ TOUR 2025\n",
            "Date: 2025-07-26T21:00:00\n",
            "Price: 180000\n",
            "Coordinates: ['25.3165692° S', '57.5883495° W']\n",
            "Successfully added event: CHAYANNE - BAILEMOS OTRA VEZ TOUR 2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/divorciadisima-la-despedida\n",
            "Successfully processed: DIVORCIADISIMA  - LA DESPEDIDA\n",
            "Date: 2026-05-03T21:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2893291° S', '57.636354° W']\n",
            "Successfully added event: DIVORCIADISIMA  - LA DESPEDIDA\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/puka-fest-2025\n",
            "Successfully processed: PUKA FEST 2025\n",
            "Date: 2025-07-04T21:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2893016° S', '57.6362087° W']\n",
            "Successfully added event: PUKA FEST 2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/renaciendo-el-poder-de-la-resiliencia-luis-montanaro\n",
            "Successfully processed: RENACIENDO: EL PODER DE LA RESILIENCIA - LUIS MONTANARO\n",
            "Date: 2025-06-26T10:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.1922233° S', '57.5087339° W']\n",
            "Successfully added event: RENACIENDO: EL PODER DE LA RESILIENCIA - LUIS MONTANARO\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/te-pido-mil-disculpas-stand-up\n",
            "Successfully processed: \"TE PIDO MIL DISCULPAS\" STAND UP\n",
            "Date: 2025-08-20T20:30:00\n",
            "Price: None\n",
            "Coordinates: ['25.2729334° S', '57.5644235° W']\n",
            "Successfully added event: \"TE PIDO MIL DISCULPAS\" STAND UP\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/lisandro-aristimuno-en-asuncion\n",
            "Successfully processed: LISANDRO ARISTIMUÑO EN ASUNCION\n",
            "Date: 2025-06-07T20:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.2853831° S', '57.6234262° W']\n",
            "Successfully added event: LISANDRO ARISTIMUÑO EN ASUNCION\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/japuka-akupe\n",
            "Successfully processed: JAPUKA´ACUPE – FESTIVAL DE HUMOR\n",
            "Date: 2025-06-14T21:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.3867039° S', '57.1393846° W']\n",
            "Successfully added event: JAPUKA´ACUPE – FESTIVAL DE HUMOR\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/paulo-londra\n",
            "Successfully processed: PAULO LONDRA\n",
            "Date: 2025-05-29T17:29:59.173566\n",
            "Price: 0\n",
            "Coordinates: ['25.2744463° S', '57.6445734° W']\n",
            "Successfully added event: PAULO LONDRA\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/groovers-rec-10-matias-sundblad\n",
            "Successfully processed: GROOVERS REC 10 - MATIAS SUNDBLAD\n",
            "Date: 2025-05-31T22:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.294089° S', '57.5772271° W']\n",
            "Successfully added event: GROOVERS REC 10 - MATIAS SUNDBLAD\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/barca-escola-futsal-2025\n",
            "Successfully processed: BARÇA ESCOLA FUTSAL 2025\n",
            "Date: 2025-07-14T08:30:00\n",
            "Price: 3120000\n",
            "Coordinates: ['25.25957° S', '57.5318931° W']\n",
            "Successfully added event: BARÇA ESCOLA FUTSAL 2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/congreso-internacional-eduvet\n",
            "Successfully processed: CONGRESO INTERNACIONAL EDUVET\n",
            "Date: 2025-09-07T08:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.3693014° S', '57.6334951° W']\n",
            "Successfully added event: CONGRESO INTERNACIONAL EDUVET\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/party-glow-faces\n",
            "Successfully processed: PARTY GLOW - FACES\n",
            "Date: 2025-05-31T22:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.305585° S', '57.534851° W']\n",
            "Successfully added event: PARTY GLOW - FACES\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/expo-vino-2025\n",
            "Successfully processed: EXPO VINO 2025\n",
            "Date: 2025-06-24T18:30:00\n",
            "Price: None\n",
            "Coordinates: ['25.2724079° S', '57.6105512° W']\n",
            "Successfully added event: EXPO VINO 2025\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/amor-y-crecimiento-luis-montanaro\n",
            "Successfully processed: AMOR Y CRECIMIENTO - LUIS MONTANARO\n",
            "Date: 2025-06-05T10:00:00\n",
            "Price: None\n",
            "Coordinates: ['25.1922233° S', '57.5087339° W']\n",
            "Successfully added event: AMOR Y CRECIMIENTO - LUIS MONTANARO\n",
            "\n",
            "Fetching details for event: https://ticketea.com.py/events/eladio-don-kbrn-world-tour-paraguay-2025\n",
            "Successfully processed: ELADIO | DON KBRN WORLD TOUR - PARAGUAY 2025\n",
            "Date: 2025-10-25T19:00:00\n",
            "Price: 120000\n",
            "Coordinates: ['25.3165692° S', '57.5883495° W']\n",
            "Successfully added event: ELADIO | DON KBRN WORLD TOUR - PARAGUAY 2025\n",
            "\n",
            "Scraping completed!\n",
            "Total events processed: 35\n",
            "File saved: output/ticketea_events_20250529_173021.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JOdAPkZXlCWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jsl7uVo9-er8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETL"
      ],
      "metadata": {
        "id": "d0V_5tSd-_Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzFqv_J__JTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update To Firestore\n"
      ],
      "metadata": {
        "id": "akD9Oc70-dsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def determine_category(event):\n",
        "    \"\"\"Determine proper category based on event data.\"\"\"\n",
        "    # Get category data from different possible sources\n",
        "    meta_category = (event.get('meta', {}).get('category') or '').lower()\n",
        "    description = (event.get('eventDescription', '') or event.get('additional_details', '') or '').lower()\n",
        "    title = (event.get('title') or '').lower()\n",
        "\n",
        "    # Define category mappings\n",
        "    category_map = {\n",
        "        'conciertos': ['concierto', 'música', 'music', 'show', 'festival', 'recital', 'banda', 'cantante'],\n",
        "        'teatro': ['teatro', 'obra', 'drama', 'comedia', 'musical'],\n",
        "        'deportes': ['carrera', 'deporte', 'partido', 'torneo', 'competencia', '5k', 'maratón'],\n",
        "        'festivales': ['festival', 'fiesta', 'party', 'carnaval', 'patronales'],\n",
        "        'educación': ['curso', 'taller', 'workshop', 'capacitación', 'seminario', 'congreso', 'colación'],\n",
        "        'arte': ['arte', 'exposición', 'galería', 'museum', 'exhibición'],\n",
        "        'familiar': ['familiar', 'family', 'niños', 'kids'],\n",
        "        'urbano': ['dj', 'electrónica', 'reggaeton', 'rap', 'trap', 'hip hop'],\n",
        "        'cultural': ['cultural', 'tradición', 'folklore']\n",
        "    }\n",
        "\n",
        "    # Text to check\n",
        "    text_to_check = f\"{title} {description} {meta_category}\"\n",
        "\n",
        "    # Check each category's keywords\n",
        "    for category, keywords in category_map.items():\n",
        "        if any(keyword in text_to_check for keyword in keywords):\n",
        "            return category\n",
        "\n",
        "    # Default category\n",
        "    return 'otros'\n",
        "\n",
        "def parse_date_string(date_str):\n",
        "    \"\"\"Parse various date string formats and return datetime object.\"\"\"\n",
        "    if not date_str:\n",
        "        return datetime.now()\n",
        "\n",
        "    try:\n",
        "        # Clean up the date string\n",
        "        date_str = date_str.lower()\n",
        "\n",
        "        # Remove day names\n",
        "        date_str = re.sub(r'^(lunes|martes|miércoles|jueves|viernes|sábado|domingo)\\s+', '', date_str)\n",
        "\n",
        "        # Fix common scraping issues in month names\n",
        "        date_str = re.sub(r'februaryruary|februaryuary', 'february', date_str)\n",
        "        date_str = re.sub(r'marchch', 'march', date_str)\n",
        "        date_str = re.sub(r'julyy', 'july', date_str)\n",
        "        date_str = re.sub(r'septembertember', 'september', date_str)\n",
        "\n",
        "        # Convert Spanish months to English\n",
        "        month_map = {\n",
        "            'enero': 'january', 'febrero': 'february', 'marzo': 'march',\n",
        "            'abril': 'april', 'mayo': 'may', 'junio': 'june',\n",
        "            'julio': 'july', 'agosto': 'august', 'septiembre': 'september',\n",
        "            'octubre': 'october', 'noviembre': 'november', 'diciembre': 'december',\n",
        "            'ene': 'january', 'feb': 'february', 'mar': 'march',\n",
        "            'abr': 'april', 'may': 'may', 'jun': 'june',\n",
        "            'jul': 'july', 'ago': 'august', 'sep': 'september',\n",
        "            'oct': 'october', 'nov': 'november', 'dic': 'december'\n",
        "        }\n",
        "\n",
        "        for spanish, english in month_map.items():\n",
        "            date_str = re.sub(rf'\\b{spanish}\\b', english, date_str)\n",
        "\n",
        "        # Remove 'de' and other connecting words\n",
        "        date_str = re.sub(r'\\sde\\s', ' ', date_str)\n",
        "\n",
        "        # Extract date components\n",
        "        match = re.search(r'(\\d{1,2})\\s+([a-z]+)(?:\\s+(\\d{4}))?\\s*(?:-|,)?\\s*(\\d{1,2}):(\\d{2})', date_str)\n",
        "\n",
        "        if match:\n",
        "            day = int(match.group(1))\n",
        "            month = match.group(2)\n",
        "            year = int(match.group(3)) if match.group(3) else 2025\n",
        "            hour = int(match.group(4))\n",
        "            minute = int(match.group(5))\n",
        "        else:\n",
        "            match = re.search(r'(\\d{1,2})\\s+([a-z]+)(?:\\s+(\\d{4}))?', date_str)\n",
        "            if not match:\n",
        "                return datetime.now()\n",
        "\n",
        "            day = int(match.group(1))\n",
        "            month = match.group(2)\n",
        "            year = int(match.group(3)) if match.group(3) else 2025\n",
        "            hour = 0\n",
        "            minute = 0\n",
        "\n",
        "        month_nums = {\n",
        "            'january': 1, 'february': 2, 'march': 3, 'april': 4,\n",
        "            'may': 5, 'june': 6, 'july': 7, 'august': 8,\n",
        "            'september': 9, 'october': 10, 'november': 11, 'december': 12\n",
        "        }\n",
        "\n",
        "        month_num = month_nums.get(month.lower())\n",
        "        if not month_num:\n",
        "            raise ValueError(f\"Invalid month: {month}\")\n",
        "\n",
        "        return datetime(year, month_num, day, hour, minute)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date '{date_str}': {str(e)}\")\n",
        "        return datetime.now()\n",
        "\n",
        "def transform_event(event):\n",
        "    \"\"\"Transform event data to Firestore format.\"\"\"\n",
        "    # Parse dates\n",
        "    start_time = parse_date_string(event.get('date') or event.get('event_hours', ''))\n",
        "\n",
        "    # Clean up location data\n",
        "    location = event.get('event_location', '') or event.get('location', '')\n",
        "    if location == 'No Location':\n",
        "        location = 'Asunción'\n",
        "\n",
        "    # Extract coordinates\n",
        "    lat = -25.2867  # Default coordinates for Asunción\n",
        "    lng = -57.3333\n",
        "\n",
        "    # For Tuti events\n",
        "    if 'details' in event and event['details'].get('coordinates'):\n",
        "        coordinates = event['details']['coordinates']\n",
        "        if coordinates.get('latitude') and coordinates.get('longitude'):\n",
        "            try:\n",
        "                lat = float(coordinates['latitude'])\n",
        "                lng = float(coordinates['longitude'])\n",
        "            except (ValueError, TypeError):\n",
        "                pass\n",
        "\n",
        "    # For Ticketea events\n",
        "    elif event.get('coordinates'):\n",
        "        coordinates = event['coordinates']\n",
        "        if coordinates.get('latitude') and coordinates.get('longitude'):\n",
        "            try:\n",
        "                lat = float(coordinates['latitude'])\n",
        "                lng = float(coordinates['longitude'])\n",
        "            except (ValueError, TypeError):\n",
        "                pass\n",
        "\n",
        "    # Ensure numeric values\n",
        "    try:\n",
        "        raw_price = event.get('price', '') or event.get('meta', {}).get('price', '0')\n",
        "        price = int(float(str(raw_price).replace('PYG', '').replace('.', '').strip()))\n",
        "    except (ValueError, TypeError):\n",
        "        price = 0\n",
        "\n",
        "    # Get category\n",
        "    event_category = determine_category(event)\n",
        "\n",
        "    transformed = {\n",
        "        \"eventId\": event.get('meta', {}).get('id', '') or event.get('url', '').split('/')[-1],\n",
        "        \"eventName\": event.get('title', ''),\n",
        "        \"eventPhoto\": event.get('image_url', ''),\n",
        "        \"createdTime\": \"SERVER_TIMESTAMP\",\n",
        "        \"eventStartTime\": start_time,\n",
        "        \"eventEndTime\": start_time,  # Same as start time since we don't have end time\n",
        "        \"priceTicket\": price,\n",
        "        \"registeredUsers\": [],\n",
        "        \"street\": event.get('event_address', ''),\n",
        "        \"city\": location,\n",
        "        \"zipcode\": \"\",\n",
        "        \"country\": \"PY\",\n",
        "        \"uid\": \"rBU6BGqtw1QAMeXOxUEktFtfVan1\",\n",
        "        \"eventDescription\": event.get('additional_details', '') or event.get('meta', {}).get('description', ''),\n",
        "        \"eventCategory\": event_category,\n",
        "        \"latlong\": {\"latitude\": lat, \"longitude\": lng},\n",
        "        \"eventPrice\": price,\n",
        "        \"name\": event.get('title', ''),\n",
        "        \"source\": \"ticketea\" if 'ticketea' in str(event.get('url', '')) else \"tuti\"\n",
        "    }\n",
        "\n",
        "    print(f\"Processing {transformed['eventName']} - Category: {event_category}\")\n",
        "    return transformed\n",
        "\n",
        "def process_events(json_file):\n",
        "    \"\"\"Process events from JSON file and transform to Firestore format.\"\"\"\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        events = json.load(f)\n",
        "    return [transform_event(event) for event in events]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main process to transform events and save output.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Process both Ticketea and Tuti events\n",
        "    tuti_path = '/content/output/tuti_events_20250112_235505.json'\n",
        "    ticketea_path = '/content/output/ticketea_events_20250112_235058.json'\n",
        "\n",
        "    # Load and transform Tuti events\n",
        "    tuti_events = process_events(tuti_path)\n",
        "    ticketea_events = process_events(ticketea_path)\n",
        "\n",
        "    # Combine all events\n",
        "    all_events = tuti_events + ticketea_events\n",
        "\n",
        "    # Save transformed events\n",
        "    output_path = f'/content/output/processed_events_{timestamp}.json'\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_events, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    print(f\"Successfully processed {len(all_events)} events\")\n",
        "    print(f\"Output saved to: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z9q9hvxjLPK",
        "outputId": "fc35beeb-a3f1-46ff-82a0-5c1a0a955016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Super Copa - Category: otros\n",
            "Processing Combos Navideños - Category: otros\n",
            "Processing Carnaval Encarnaceno 2025 - Category: festivales\n",
            "Processing Conferencia de Sanidad Emocional \"Izzanami Martinez\" - Category: otros\n",
            "Processing DUKI en Concierto 5 de Abril - Category: conciertos\n",
            "Processing Carrera de las chicas 5 K - Category: deportes\n",
            "Processing FIESTA RETROVISOR VERANO - Category: festivales\n",
            "Processing So Pra Contrariar - Category: otros\n",
            "Processing Super Copa - Category: otros\n",
            "Processing Combos Navideños - Category: otros\n",
            "Processing Carnaval Encarnaceno 2025 - Category: festivales\n",
            "Processing Conferencia de Sanidad Emocional \"Izzanami Martinez\" - Category: otros\n",
            "Processing DUKI en Concierto 5 de Abril - Category: conciertos\n",
            "Processing Festival del Ykua Bolaños - Category: conciertos\n",
            "Processing Sepultura - Category: otros\n",
            "Processing Carrera de las chicas 5 K - Category: deportes\n",
            "Processing Fansrock - Category: otros\n",
            "Processing FIESTA RETROVISOR VERANO - Category: festivales\n",
            "Processing So Pra Contrariar - Category: otros\n",
            "Processing FLOU En concierto - Category: conciertos\n",
            "Processing The link - Category: otros\n",
            "Processing #TODOSXBOLLO - Category: otros\n",
            "Processing Documental Tito Torres, más allá de los 90 - Category: otros\n",
            "Processing Curso Power BI desde Cero - En vivo - Category: educación\n",
            "Processing Power BI Analytics + Power BI Reporting - Category: otros\n",
            "Processing Gestión de Datos + PBI Analytics + PBI Reporting - Category: otros\n",
            "Processing TECHNASIA MAMBO DOS22 - Category: otros\n",
            "Processing BUSES A PISO 21 & CHAPA C DESDE ASUNCIÓN A SAN BERNARDINO - Category: conciertos\n",
            "Processing TAN BIONICA - LA ULTIMA NOCHE MAGICA TOUR - Category: conciertos\n",
            "Processing SUNRISE OLOGRAM - Category: festivales\n",
            "Processing STEVE AOKI - Category: otros\n",
            "Processing BUSES A TAN BIONICA LA ULTIMA NOCHE MAGICA - DESDE CIUDAD DEL ESTE - Category: conciertos\n",
            "Processing CONGRESO DIAMANTE 2025 –“ Renuevo el Pacto” - Category: deportes\n",
            "Processing NELSON VELAZQUEZ 2025 - Category: conciertos\n",
            "Processing DUKI - AMERI WORLD TOUR 2025 - DIA 2 - Category: conciertos\n",
            "Processing VELVET SUMMER - TRUENO & DEI V. - Category: otros\n",
            "Processing BENJAMIN AMADEO EN ASUNCIÓN - PY ROCK FEST - Category: conciertos\n",
            "Processing COLACION FICDE PY - Category: otros\n",
            "Processing DOS-22 Presenta VICTORIA MUSSI B2B AMANDA MUSSI (ALL NIGHT LONG) - Category: festivales\n",
            "Processing CHAYANNE - BAILEMOS OTRA VEZ TOUR 2025 - Category: conciertos\n",
            "Processing RETRO FEST - Category: festivales\n",
            "Processing LOS VERDADEROS PRESENTAN: VIUDO POR ERROR 2025 - Category: teatro\n",
            "Processing PATRONALES ITA 2025 - SPTVO ITEÑO - Category: conciertos\n",
            "Processing AFTER SAFARI X MAMBO - Category: festivales\n",
            "Processing VELVET SUMMER - PISO 21 & CHAPA C - Category: otros\n",
            "Processing KANY GARCIA EN PARAGUAY - Category: conciertos\n",
            "Processing CONFERENCIA BOOT CAMP CON ITIEL ARROYO - Category: educación\n",
            "Processing BUSES A CUMBIASONICO 2025 DESDE ASUNCIÓN A SAN BERNARDINO - Category: conciertos\n",
            "Processing CONGRESO INTERNACIONAL EDUVET - Category: educación\n",
            "Processing CUMBIASONICO SANBER 2025 - Category: otros\n",
            "Processing RQP SESSIONS - Category: conciertos\n",
            "Processing BUSES A SOLOMUN 2025 ASUNCIÓN - SAN BERNARDINO - Category: conciertos\n",
            "Processing KATARRO VANDALIKO 20 AÑOS DE TE DE ESTILO - Category: conciertos\n",
            "Processing PATRONALES DE PIRIBEBUY 2025 - Category: festivales\n",
            "Processing DUKI - AMERI WORLD TOUR 2025 - DIA 1 - Category: conciertos\n",
            "Processing BUSES A FLASHBACK FEST DESDE ASUNCIÓN A SAN BERNARDINO - Category: conciertos\n",
            "Processing SOLOMUN - Category: otros\n",
            "Processing FLASHBACK FEST 2025 - Category: conciertos\n",
            "Processing CAMILO - NUESTRO LUGAR FELIZ TOUR 2025 - Category: conciertos\n",
            "Processing BUSES A VELVET SUMMER - TRUENO & DEI V.  A SAN BERNARDINO - Category: conciertos\n",
            "Processing LA HUMANIDAD - Category: conciertos\n",
            "Processing BUSES A TAN BIONICA LA ULTIMA NOCHE MAGICA  - DESDE ENCARNACION - Category: conciertos\n",
            "Processing SUMMER SUNSET - ENC - Category: festivales\n",
            "Successfully processed 63 events\n",
            "Output saved to: /content/output/processed_events_20250113_005829.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_CLOUD_PROJECT'] = 'vamospy-discovery-vf7nj4'"
      ],
      "metadata": {
        "id": "zaPkWOLEo-sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "boqAK5dr-wl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def parse_date_string(date_str):\n",
        "    \"\"\"Parse various date string formats and return datetime object.\"\"\"\n",
        "    if not date_str:\n",
        "        return datetime.now()\n",
        "\n",
        "    try:\n",
        "        # Clean up the date string\n",
        "        date_str = date_str.lower()\n",
        "\n",
        "        # Remove day names\n",
        "        date_str = re.sub(r'^(lunes|martes|miércoles|jueves|viernes|sábado|domingo)\\s+', '', date_str)\n",
        "\n",
        "        # Fix common scraping issues in month names\n",
        "        date_str = re.sub(r'februaryruary|februaryuary', 'february', date_str)\n",
        "        date_str = re.sub(r'marchch', 'march', date_str)\n",
        "        date_str = re.sub(r'julyy', 'july', date_str)\n",
        "        date_str = re.sub(r'septembertember', 'september', date_str)\n",
        "\n",
        "        # Convert Spanish months to English\n",
        "        month_map = {\n",
        "            'enero': 'january',\n",
        "            'febrero': 'february',\n",
        "            'marzo': 'march',\n",
        "            'abril': 'april',\n",
        "            'mayo': 'may',\n",
        "            'junio': 'june',\n",
        "            'julio': 'july',\n",
        "            'agosto': 'august',\n",
        "            'septiembre': 'september',\n",
        "            'octubre': 'october',\n",
        "            'noviembre': 'november',\n",
        "            'diciembre': 'december',\n",
        "            'ene': 'january',\n",
        "            'feb': 'february',\n",
        "            'mar': 'march',\n",
        "            'abr': 'april',\n",
        "            'may': 'may',\n",
        "            'jun': 'june',\n",
        "            'jul': 'july',\n",
        "            'ago': 'august',\n",
        "            'sep': 'september',\n",
        "            'oct': 'october',\n",
        "            'nov': 'november',\n",
        "            'dic': 'december'\n",
        "        }\n",
        "\n",
        "        for spanish, english in month_map.items():\n",
        "            date_str = re.sub(rf'\\b{spanish}\\b', english, date_str)\n",
        "\n",
        "        # Remove 'de' and other connecting words\n",
        "        date_str = re.sub(r'\\sde\\s', ' ', date_str)\n",
        "\n",
        "        # Extract date components\n",
        "        match = re.search(r'(\\d{1,2})\\s+([a-z]+)(?:\\s+(\\d{4}))?\\s*(?:-|,)?\\s*(\\d{1,2}):(\\d{2})', date_str)\n",
        "\n",
        "        if match:\n",
        "            day = int(match.group(1))\n",
        "            month = match.group(2)\n",
        "            year = int(match.group(3)) if match.group(3) else 2025\n",
        "            hour = int(match.group(4))\n",
        "            minute = int(match.group(5))\n",
        "        else:\n",
        "            match = re.search(r'(\\d{1,2})\\s+([a-z]+)(?:\\s+(\\d{4}))?', date_str)\n",
        "            if not match:\n",
        "                return datetime.now()\n",
        "\n",
        "            day = int(match.group(1))\n",
        "            month = match.group(2)\n",
        "            year = int(match.group(3)) if match.group(3) else 2025\n",
        "            hour = 0\n",
        "            minute = 0\n",
        "\n",
        "        month_nums = {\n",
        "            'january': 1, 'february': 2, 'march': 3, 'april': 4,\n",
        "            'may': 5, 'june': 6, 'july': 7, 'august': 8,\n",
        "            'september': 9, 'october': 10, 'november': 11, 'december': 12\n",
        "        }\n",
        "\n",
        "        month_num = month_nums.get(month.lower())\n",
        "        if not month_num:\n",
        "            raise ValueError(f\"Invalid month: {month}\")\n",
        "\n",
        "        return datetime(year, month_num, day, hour, minute)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date '{date_str}': {str(e)}\")\n",
        "        return datetime.now()\n",
        "\n",
        "def transform_event(event):\n",
        "    \"\"\"Transform event data to Firestore format.\"\"\"\n",
        "    # Parse dates\n",
        "    start_time = parse_date_string(event.get('date') or event.get('event_hours', ''))\n",
        "\n",
        "    # Clean up location data\n",
        "    location = event.get('event_location', '') or event.get('location', '')\n",
        "    if location == 'No Location':\n",
        "        location = 'Asunción'\n",
        "\n",
        "    # Extract coordinates\n",
        "    lat = -25.2867  # Default coordinates for Asunción\n",
        "    lng = -57.3333\n",
        "\n",
        "    # For Tuti events\n",
        "    if 'details' in event and event['details'].get('coordinates'):\n",
        "        coordinates = event['details']['coordinates']\n",
        "        if coordinates.get('latitude') and coordinates.get('longitude'):\n",
        "            lat = float(coordinates['latitude'])\n",
        "            lng = float(coordinates['longitude'])\n",
        "\n",
        "    # For Ticketea events\n",
        "    elif event.get('coordinates'):\n",
        "        coordinates = event['coordinates']\n",
        "        if coordinates.get('latitude') and coordinates.get('longitude'):\n",
        "            lat = float(coordinates['latitude'])\n",
        "            lng = float(coordinates['longitude'])\n",
        "\n",
        "    # Ensure numeric values\n",
        "    try:\n",
        "        raw_price = event.get('price', '') or event.get('meta', {}).get('price', '0')\n",
        "        price = int(float(str(raw_price).replace('PYG', '').replace('.', '').strip()))\n",
        "    except (ValueError, TypeError):\n",
        "        price = 0\n",
        "\n",
        "    transformed = {\n",
        "        \"eventId\": event.get('meta', {}).get('id', '') or event.get('url', '').split('/')[-1],\n",
        "        \"eventName\": event.get('title', ''),\n",
        "        \"eventPhoto\": event.get('image_url', ''),\n",
        "        \"createdTime\": \"SERVER_TIMESTAMP\",\n",
        "        \"eventStartTime\": start_time,\n",
        "        \"eventEndTime\": start_time,  # Same as start time since we don't have end time\n",
        "        \"priceTicket\": price,\n",
        "        \"registeredUsers\": [],\n",
        "        \"street\": event.get('event_address', ''),\n",
        "        \"city\": location,\n",
        "        \"zipcode\": \"\",\n",
        "        \"country\": \"PY\",\n",
        "        \"uid\": \"users/rBU6BGqtw1QAMeXOxUEktFtfVan1\",\n",
        "        \"eventDescription\": event.get('additional_details', '') or event.get('meta', {}).get('description', ''),\n",
        "        \"eventCategory\": event.get('meta', {}).get('category', ''),\n",
        "        \"latlong\": {\"latitude\": lat, \"longitude\": lng},\n",
        "        \"eventPrice\": price,\n",
        "        \"name\": event.get('title', ''),\n",
        "        \"source\": \"ticketea\" if 'ticketea' in str(event.get('url', '')) else \"tuti\"\n",
        "    }\n",
        "\n",
        "    print(f\"Processing {transformed['eventName']} - Coordinates: {lat}, {lng}\")\n",
        "    return transformed\n",
        "\n",
        "def process_events(json_file):\n",
        "    \"\"\"Process events from JSON file and transform to Firestore format.\"\"\"\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        events = json.load(f)\n",
        "    return [transform_event(event) for event in events]\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main process to transform events and save output.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Process both Ticketea and Tuti events\n",
        "    tuti_path = '/content/output/tuti_events_20250107_223214.json'\n",
        "    ticketea_path = '/content/output/ticketea_events_20250107_214401.json'\n",
        "\n",
        "    # Load and transform Tuti events\n",
        "    tuti_events = process_events(tuti_path)\n",
        "    ticketea_events = process_events(ticketea_path)\n",
        "\n",
        "    # Combine all events\n",
        "    all_events = tuti_events + ticketea_events\n",
        "\n",
        "    # Save transformed events\n",
        "    output_path = f'/content/output/processed_events_{timestamp}.json'\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_events, f, ensure_ascii=False, indent=2, default=str)\n",
        "\n",
        "    print(f\"Successfully processed {len(all_events)} events\")\n",
        "    print(f\"Output saved to: {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "IdRlebZaA32u",
        "outputId": "0ada9302-1534-4d6d-9a6f-5cf1bbb014e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-532a758d6259>\u001b[0m in \u001b[0;36m<cell line: 230>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-532a758d6259>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Load and transform Tuti events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mtuti_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuti_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mticketea_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticketea_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-532a758d6259>\u001b[0m in \u001b[0;36mprocess_events\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-532a758d6259>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-532a758d6259>\u001b[0m in \u001b[0;36mtransform_event\u001b[0;34m(event)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# Determine category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mevent_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     transformed = {\n",
            "\u001b[0;32m<ipython-input-26-532a758d6259>\u001b[0m in \u001b[0;36mdetermine_category\u001b[0;34m(event)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Get all possible category information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mmeta_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'details'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eventDescription'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'additional_details'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bc5g0co2-yzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "def prepare_firestore_data(event_data):\n",
        "    \"\"\"Prepare data for Firestore by converting data types appropriately.\"\"\"\n",
        "    data = event_data.copy()\n",
        "\n",
        "    # Convert datetime strings to datetime objects\n",
        "    if isinstance(data.get('eventStartTime'), str):\n",
        "        try:\n",
        "            data['eventStartTime'] = datetime.fromisoformat(data['eventStartTime'])\n",
        "        except (ValueError, TypeError):\n",
        "            data['eventStartTime'] = datetime.now()\n",
        "\n",
        "    if isinstance(data.get('eventEndTime'), str):\n",
        "        try:\n",
        "            data['eventEndTime'] = datetime.fromisoformat(data['eventEndTime'])\n",
        "        except (ValueError, TypeError):\n",
        "            data['eventEndTime'] = datetime.now()\n",
        "\n",
        "    # Handle ServerTimestamp\n",
        "    if data.get('createdTime') == 'SERVER_TIMESTAMP':\n",
        "        data['createdTime'] = firestore.SERVER_TIMESTAMP\n",
        "\n",
        "    # Convert latlong to GeoPoint if it's a dict\n",
        "    if isinstance(data.get('latlong'), dict):\n",
        "        lat = float(data['latlong'].get('latitude', -25.2867))\n",
        "        lng = float(data['latlong'].get('longitude', -57.3333))\n",
        "        data['latlong'] = firestore.GeoPoint(lat, lng)\n",
        "\n",
        "    # Ensure registeredUsers is a list\n",
        "    if not isinstance(data.get('registeredUsers'), list):\n",
        "        data['registeredUsers'] = []\n",
        "\n",
        "    # Ensure numeric fields are integers\n",
        "    try:\n",
        "        data['priceTicket'] = int(data.get('priceTicket', 0))\n",
        "    except (ValueError, TypeError):\n",
        "        data['priceTicket'] = 0\n",
        "\n",
        "    try:\n",
        "        data['eventPrice'] = int(data.get('eventPrice', 0))\n",
        "    except (ValueError, TypeError):\n",
        "        data['eventPrice'] = 0\n",
        "\n",
        "    return data\n",
        "\n",
        "def upload_to_firestore(json_path, cred_path):\n",
        "    \"\"\"\n",
        "    Upload processed events to Firestore.\n",
        "\n",
        "    Args:\n",
        "        json_path: Path to the processed events JSON file\n",
        "        cred_path: Path to the Firebase credentials JSON file\n",
        "    \"\"\"\n",
        "    # Configure logging\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    try:\n",
        "        # Set project ID\n",
        "        os.environ['GOOGLE_CLOUD_PROJECT'] = 'vamospy-discovery-vf7nj4'\n",
        "\n",
        "        # Initialize Firebase\n",
        "        if not firebase_admin._apps:\n",
        "            cred = credentials.Certificate(cred_path)\n",
        "            firebase_admin.initialize_app(cred)\n",
        "\n",
        "        db = firestore.client()\n",
        "        logger.info(\"Successfully connected to Firestore\")\n",
        "\n",
        "        # Read the processed events\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            events = json.load(f)\n",
        "        logger.info(f\"Loaded {len(events)} events from {json_path}\")\n",
        "\n",
        "        # Process events in batches\n",
        "        batch_size = 400\n",
        "        total_uploaded = 0\n",
        "        total_errors = 0\n",
        "        batch = db.batch()\n",
        "        count = 0\n",
        "\n",
        "        for event in events:\n",
        "            try:\n",
        "                # Prepare data for Firestore\n",
        "                event_data = prepare_firestore_data(event)\n",
        "\n",
        "                # Get document ID\n",
        "                doc_id = event_data.get('eventId')\n",
        "                if not doc_id:\n",
        "                    logger.warning(f\"Skipping event with missing eventId: {event}\")\n",
        "                    continue\n",
        "\n",
        "                # Create reference and add to batch\n",
        "                doc_ref = db.collection('events').document(doc_id)\n",
        "                batch.set(doc_ref, event_data)\n",
        "                count += 1\n",
        "                total_uploaded += 1\n",
        "\n",
        "                # Commit batch when limit reached\n",
        "                if count >= batch_size:\n",
        "                    batch.commit()\n",
        "                    logger.info(f\"Committed batch of {count} events. Total uploaded: {total_uploaded}\")\n",
        "                    batch = db.batch()\n",
        "                    count = 0\n",
        "\n",
        "            except Exception as e:\n",
        "                total_errors += 1\n",
        "                logger.error(f\"Error processing event {event.get('eventId', 'UNKNOWN')}: {str(e)}\")\n",
        "                if total_errors > 50:\n",
        "                    logger.error(\"Too many errors. Stopping upload.\")\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "        # Commit remaining batch\n",
        "        if count > 0:\n",
        "            batch.commit()\n",
        "            logger.info(f\"Committed final batch of {count} events\")\n",
        "\n",
        "        logger.info(f\"Upload completed. Total events uploaded: {total_uploaded}, Total errors: {total_errors}\")\n",
        "        return total_uploaded, total_errors\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Critical error during upload: {str(e)}\")\n",
        "        return 0, 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    json_path = '/content/output/processed_events_20250108_001716.json'\n",
        "    cred_path = '/content/vamospy-discovery-vf7nj4-89f04f350c3f.json'\n",
        "\n",
        "    if not os.path.exists(cred_path):\n",
        "        print(f\"Error: Service account key not found at {cred_path}\")\n",
        "    elif not os.path.exists(json_path):\n",
        "        print(f\"Error: Processed events file not found at {json_path}\")\n",
        "    else:\n",
        "        total_uploaded, total_errors = upload_to_firestore(json_path, cred_path)\n",
        "        print(f\"Upload complete: {total_uploaded} uploaded, {total_errors} errors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6doTpVh0HZsC",
        "outputId": "0ffb2b0d-02de-4718-d8b4-73ec50f1a3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload complete: 61 uploaded, 0 errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import logging\n",
        "import os\n",
        "\n",
        "def update_uids_in_firestore():\n",
        "    \"\"\"Update all UIDs in Firestore events collection.\"\"\"\n",
        "    try:\n",
        "        # Get db reference\n",
        "        db = firestore.client()\n",
        "\n",
        "        # Get all documents\n",
        "        events_ref = db.collection('events')\n",
        "        docs = events_ref.stream()\n",
        "\n",
        "        # Process updates directly (without batch)\n",
        "        updated = 0\n",
        "        for doc in docs:\n",
        "            try:\n",
        "                events_ref.document(doc.id).set({\n",
        "                    'uid': '/users/rBU6BGqtw1QAMeXOxUEktFtfVan1'\n",
        "                }, merge=True)\n",
        "                updated += 1\n",
        "                print(f\"Updated document {doc.id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error updating document {doc.id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully updated {updated} documents\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set project ID\n",
        "    os.environ['GOOGLE_CLOUD_PROJECT'] = 'vamospy-discovery-vf7nj4'\n",
        "\n",
        "    # Initialize Firebase\n",
        "    if not firebase_admin._apps:\n",
        "        cred = credentials.Certificate('/content/vamospy-discovery-vf7nj4-89f04f350c3f.json')\n",
        "        firebase_admin.initialize_app(cred)\n",
        "\n",
        "    update_uids_in_firestore()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAMGRNmnUXv3",
        "outputId": "02a15157-0464-4a49-e000-32b9f6c57bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated document 011e5e95-a5d3-47e1-9e08-0e905a1fa444\n",
            "Updated document 01c43b58-e459-4252-a958-493bbb9f6824\n",
            "Updated document 0329c5c0-f703-47ed-a032-e77c2685aa8f\n",
            "Updated document 1054284f-0bbe-4238-bbf9-81c8cff4d7f8\n",
            "Updated document 118c6b5d-b1d0-4280-8fd2-22a9a8df1d06\n",
            "Updated document 11b27e4e-0732-4a97-aba5-df57d8357ec4\n",
            "Updated document 15eb83c8-fa03-4c14-bad0-5ba2303f9368\n",
            "Updated document 27c97dbb-2c58-45c3-9b7b-1fa4c7495016\n",
            "Updated document 43410aa5-ed86-4f77-a935-feed745e0f81\n",
            "Updated document 50-aniversario-credivill\n",
            "Updated document 57fe698e-9f5b-43e2-b154-29f9a4da8a61\n",
            "Updated document 59845c62-2bee-4336-8078-58c8e043406c\n",
            "Updated document 6aadce01-6c3d-44ea-b2d2-086b5bfbd59c\n",
            "Updated document 708ddbfe-64c7-4a4f-9a0f-bc6acefa949f\n",
            "Updated document 7113c965-4ec0-4fbd-8460-9cdf2229448e\n",
            "Updated document 8tdibgsOzODv46hM2Hmk\n",
            "Updated document 909a56bd-498c-4ccf-be9c-65a8b7269f58\n",
            "Updated document 9452cf0a-8123-4b65-9d1a-f305c37232f3\n",
            "Updated document 9825849c-0da3-4a47-9c84-811521889d4e\n",
            "Updated document 98e872c1-2bb5-4b40-898d-4d324aa90b92\n",
            "Updated document EacVahsbX0CNzGCbqqJ2\n",
            "Updated document a4c068f5-845d-42ea-9c90-e1ac78570347\n",
            "Updated document a53c59c4-d71b-4b24-bb24-b8c6ab96b442\n",
            "Updated document ad6bdd84-e9fc-4b30-8c41-3bee7236a704\n",
            "Updated document ad7d50a0-35cf-4776-a802-77517de12338\n",
            "Updated document aee0d598-8138-4910-b27e-8c875410f97f\n",
            "Updated document aniversario-faces\n",
            "Updated document atemporal-el-musical-funcion-azul\n",
            "Updated document atemporal-el-musical-funcion-naranja\n",
            "Updated document b5f74cf5-f268-4318-977e-d7abb7314750\n",
            "Updated document ba1f95b9-0760-4d14-aa0c-39d83f03d537\n",
            "Updated document baae7e54-ece4-4ec5-968d-48a9564e3299\n",
            "Updated document back-to-noventon-de-faces-mix\n",
            "Updated document bbc62e14-d6e6-43f3-aad0-d89ee3525716\n",
            "Updated document beatwave-presenta-new-moon-2025-cde\n",
            "Updated document benjamin-amadeo-en-asuncion-py-rock-fest\n",
            "Updated document botanico\n",
            "Updated document c083ba9d-351f-4286-8c3c-8af37dbc7e08\n",
            "Updated document c41e6ff6-5cdc-4fbd-86ba-cccc9dced524\n",
            "Updated document c900e0df-eb6d-44e1-837b-2f39078785b0\n",
            "Updated document calor-tropical-refugio-de-amor-en-vivo-dubai\n",
            "Updated document carnaval-encarnaceno-2025\n",
            "Updated document carrera-de-las-chicas-5-k\n",
            "Updated document chayanne-bailemos-otra-vez-tour-2025\n",
            "Updated document choppfest-caaguazu-2024\n",
            "Updated document colacion-ficde-py\n",
            "Updated document combos-navidenos\n",
            "Updated document conectando-almas\n",
            "Updated document conferencia-de-sanidad-emocional-izzanami-martinez\n",
            "Updated document congreso-diamante-renuevo-el-pacto\n",
            "Updated document crisis-de-los-casi-40\n",
            "Updated document curso-power-bi-desde-cero-en-vivo\n",
            "Updated document d18b469c-993e-4ba6-8d5c-645477d19eee\n",
            "Updated document d1eba6c3-c0f9-4276-8ed6-688046ee7772\n",
            "Updated document documental-tito-torres-mas-alla-de-los-90\n",
            "Updated document dos-22-presenta-a-the-yellow-heads\n",
            "Updated document duki-ameri-world-tour-2025\n",
            "Updated document duki-ameri-world-tour-2025-dia-2\n",
            "Updated document duki-en-concierto-5-de-abril\n",
            "Updated document e4ef7bdc-cf83-451f-9925-c5fd6b5f541c\n",
            "Updated document e7c8624b-42d2-42fc-8c07-b8fb08f643c9\n",
            "Updated document e859c065-b00b-4a73-89a7-eb01a228446a\n",
            "Updated document eee7b1dc-630d-44e8-89d8-527507d0ad84\n",
            "Updated document encuentro-despues-del-ocaso\n",
            "Updated document estilo-juarez-refugio-de-amor-en-vivo\n",
            "Updated document exa-fest-2024\n",
            "Updated document f14c34f1-b037-4ae4-aae9-46c3569d4d91\n",
            "Updated document f52b7dab-bc07-41d5-91e1-dd27eb1f0f88\n",
            "Updated document fantasy-sobre-ruedas\n",
            "Updated document festival-del-ykua-bolanos\n",
            "Updated document fiesta-de-fin-de-ano-exa-san-jose\n",
            "Updated document fiesta-retrovisor-verano\n",
            "Updated document flou-en-concierto\n",
            "Updated document gestion-de-datos-pbi-analytics-pbi-reporting\n",
            "Updated document jardiversario\n",
            "Updated document katarro-vandaliko-20-anos-de-te-de-estilo\n",
            "Updated document la-fiesta-del-ano-exa-goethe\n",
            "Updated document landton-faces\n",
            "Updated document martin-garrix-en-paraguay\n",
            "Updated document origen-latido-infinito\n",
            "Updated document paraguay-animation-community-showcase\n",
            "Updated document paraguay-performance-2024\n",
            "Updated document power-bi-analytics-power-bi-reporting\n",
            "Updated document sepultura\n",
            "Updated document so-pra-contrariar\n",
            "Updated document sol-de-america-vs-spvo-ameliano\n",
            "Updated document solomun\n",
            "Updated document soy-lider\n",
            "Updated document steve-aoki\n",
            "Updated document sumo-x-pettinato\n",
            "Updated document technasia-mambo-dos22\n",
            "Updated document technotopia-part-iii\n",
            "Updated document technoxperience\n",
            "Updated document todosxbollo\n",
            "Updated document warung-tour-asuncion-2024\n",
            "Successfully updated 95 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import os\n",
        "\n",
        "def update_all_documents():\n",
        "    \"\"\"Update all documents with the working UID format.\"\"\"\n",
        "    try:\n",
        "        db = firestore.client()\n",
        "\n",
        "        # First get the working format from our known working document\n",
        "        working_doc = db.collection('events').document('a4c068f5-845d-42ea-9c90-e1ac78570347').get()\n",
        "        working_uid = working_doc.get('uid')\n",
        "\n",
        "        print(\"Using UID format:\", working_uid)\n",
        "\n",
        "        # Process in batches for efficiency\n",
        "        batch_size = 450  # Firestore limit is 500\n",
        "        batch = db.batch()\n",
        "        count = 0\n",
        "        total_updated = 0\n",
        "\n",
        "        # Get all documents\n",
        "        docs = db.collection('events').stream()\n",
        "\n",
        "        for doc in docs:\n",
        "            try:\n",
        "                if doc.id != 'a4c068f5-845d-42ea-9c90-e1ac78570347':  # Skip our reference document\n",
        "                    doc_ref = db.collection('events').document(doc.id)\n",
        "                    batch.update(doc_ref, {'uid': working_uid})\n",
        "                    count += 1\n",
        "                    total_updated += 1\n",
        "\n",
        "                    # Commit batch when size limit reached\n",
        "                    if count >= batch_size:\n",
        "                        batch.commit()\n",
        "                        print(f\"Updated batch of {count} documents. Total: {total_updated}\")\n",
        "                        batch = db.batch()\n",
        "                        count = 0\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error updating document {doc.id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Commit any remaining documents\n",
        "        if count > 0:\n",
        "            batch.commit()\n",
        "            print(f\"Updated final batch of {count} documents\")\n",
        "\n",
        "        print(f\"\\nSuccessfully updated {total_updated} documents\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set project ID\n",
        "    os.environ['GOOGLE_CLOUD_PROJECT'] = 'vamospy-discovery-vf7nj4'\n",
        "\n",
        "    # Initialize Firebase\n",
        "    if not firebase_admin._apps:\n",
        "        cred = credentials.Certificate('/content/vamospy-discovery-vf7nj4-89f04f350c3f.json')\n",
        "        firebase_admin.initialize_app(cred)\n",
        "\n",
        "    update_all_documents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993MwHDBZdxt",
        "outputId": "08e2e06e-dd75-4148-a7e0-3187b09e177d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using UID format: <google.cloud.firestore_v1.document.DocumentReference object at 0x7aa1f049d240>\n",
            "Updated final batch of 94 documents\n",
            "\n",
            "Successfully updated 94 documents\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}